{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: Object Detection with Azure Custom Vision\n",
    "\n",
    "In this lab, you'll learn how to train a custom object detection model using Azure Custom Vision. Unlike image classification which assigns labels to entire images, object detection identifies and locates specific objects within images using bounding boxes.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the difference between classification and object detection\n",
    "- Create and configure an Azure Custom Vision object detection project\n",
    "- Upload images with bounding box annotations\n",
    "- Train a custom object detection model\n",
    "- Detect objects in images and visualize results with bounding boxes\n",
    "- Evaluate model performance using confidence thresholds\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure subscription with Custom Vision resource created\n",
    "- Training and prediction keys from Azure portal\n",
    "- Images with bounding box annotations (JSON format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install azure-cognitiveservices-vision-customvision python-dotenv pillow matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import (\n",
    "    ImageFileCreateBatch, ImageFileCreateEntry, Region\n",
    ")\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Object Detection\n",
    "\n",
    "### Classification vs Object Detection:\n",
    "\n",
    "**Image Classification:**\n",
    "- Assigns a single label to an entire image\n",
    "- Output: \"This image contains an apple\"\n",
    "\n",
    "**Object Detection:**\n",
    "- Identifies and locates multiple objects within an image\n",
    "- Output: \"Apple at coordinates (x=0.2, y=0.3, width=0.15, height=0.2) with 95% confidence\"\n",
    "\n",
    "### Bounding Box Coordinates:\n",
    "- Coordinates are normalized (0-1 range)\n",
    "- **left**: Distance from left edge (0 = left, 1 = right)\n",
    "- **top**: Distance from top edge (0 = top, 1 = bottom)\n",
    "- **width**: Width of box as fraction of image width\n",
    "- **height**: Height of box as fraction of image height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Azure Custom Vision Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "env_path = 'python/train-detector/.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Get configuration settings\n",
    "training_endpoint = os.getenv('TrainingEndpoint')\n",
    "training_key = os.getenv('TrainingKey')\n",
    "prediction_endpoint = os.getenv('PredictionEndpoint', training_endpoint)\n",
    "prediction_key = os.getenv('PredictionKey', training_key)\n",
    "project_id = os.getenv('ProjectID', None)\n",
    "\n",
    "print(f\"Training Endpoint: {training_endpoint}\")\n",
    "print(f\"Prediction Endpoint: {prediction_endpoint}\")\n",
    "print(f\"Project ID: {project_id if project_id else 'Will create new project'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Authenticate Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate training client\n",
    "training_credentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\n",
    "training_client = CustomVisionTrainingClient(training_endpoint, training_credentials)\n",
    "\n",
    "# Authenticate prediction client\n",
    "prediction_credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "prediction_client = CustomVisionPredictionClient(prediction_endpoint, prediction_credentials)\n",
    "\n",
    "print(\"Clients authenticated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Object Detection Project\n",
    "\n",
    "Create a Custom Vision project specifically for object detection. Note the `domain_id` and project type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available domains\n",
    "domains = training_client.get_domains()\n",
    "\n",
    "# Find an object detection domain\n",
    "print(\"Available Object Detection Domains:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "obj_detection_domain = None\n",
    "for domain in domains:\n",
    "    if domain.type == 'ObjectDetection':\n",
    "        print(f\"  - {domain.name} (Exportable: {domain.exportable})\")\n",
    "        if obj_detection_domain is None:\n",
    "            obj_detection_domain = domain\n",
    "\n",
    "# Create or get existing project\n",
    "if project_id:\n",
    "    print(f\"\\nConnecting to existing project: {project_id}\")\n",
    "    project = training_client.get_project(project_id)\n",
    "else:\n",
    "    project_name = f\"Fruit Object Detection {uuid.uuid4().hex[:8]}\"\n",
    "    print(f\"\\nCreating new object detection project: {project_name}\")\n",
    "    project = training_client.create_project(\n",
    "        name=project_name,\n",
    "        description=\"Detect and locate fruits (apple, banana, orange) in images\",\n",
    "        domain_id=obj_detection_domain.id\n",
    "    )\n",
    "    project_id = project.id\n",
    "\n",
    "print(f\"\\nProject Details:\")\n",
    "print(f\"  Name: {project.name}\")\n",
    "print(f\"  ID: {project.id}\")\n",
    "print(f\"  Type: Object Detection\")\n",
    "print(f\"\\n⚠️  Save this Project ID: {project.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Object Tags\n",
    "\n",
    "Create tags for the types of objects you want to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define object types to detect\n",
    "tag_names = ['apple', 'banana', 'orange']\n",
    "\n",
    "# Get existing tags or create new ones\n",
    "existing_tags = training_client.get_tags(project.id)\n",
    "existing_tag_names = [tag.name for tag in existing_tags]\n",
    "\n",
    "tags = {}\n",
    "for tag_name in tag_names:\n",
    "    if tag_name in existing_tag_names:\n",
    "        tag = next(t for t in existing_tags if t.name == tag_name)\n",
    "        print(f\"Found existing tag: {tag_name}\")\n",
    "    else:\n",
    "        tag = training_client.create_tag(project.id, tag_name)\n",
    "        print(f\"Created new tag: {tag_name}\")\n",
    "    tags[tag_name] = tag\n",
    "\n",
    "print(f\"\\nTotal tags: {len(tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and Visualize Tagged Images\n",
    "\n",
    "Load the bounding box annotations from JSON file and visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tagged images data\n",
    "tagged_images_json = 'python/train-detector/tagged-images.json'\n",
    "\n",
    "with open(tagged_images_json, 'r') as f:\n",
    "    tagged_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(tagged_data['files'])} tagged images\")\n",
    "print(f\"\\nFirst image example:\")\n",
    "print(f\"  Filename: {tagged_data['files'][0]['filename']}\")\n",
    "print(f\"  Number of objects: {len(tagged_data['files'][0]['tags'])}\")\n",
    "print(f\"\\nObject details:\")\n",
    "for obj in tagged_data['files'][0]['tags']:\n",
    "    print(f\"  - {obj['tag']}: left={obj['left']:.3f}, top={obj['top']:.3f}, \"\n",
    "          f\"width={obj['width']:.3f}, height={obj['height']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bounding_boxes(image_path, bounding_boxes, title=\"Tagged Image\"):\n",
    "    \"\"\"\n",
    "    Visualize an image with bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        bounding_boxes: List of dicts with 'tag', 'left', 'top', 'width', 'height'\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Define colors for different object types\n",
    "    colors = {'apple': 'red', 'banana': 'yellow', 'orange': 'orange'}\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for bbox in bounding_boxes:\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        left = bbox['left'] * img_width\n",
    "        top = bbox['top'] * img_height\n",
    "        width = bbox['width'] * img_width\n",
    "        height = bbox['height'] * img_height\n",
    "        \n",
    "        # Create rectangle patch\n",
    "        color = colors.get(bbox['tag'], 'cyan')\n",
    "        rect = patches.Rectangle(\n",
    "            (left, top), width, height,\n",
    "            linewidth=3, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        ax.text(\n",
    "            left, top - 5,\n",
    "            bbox['tag'],\n",
    "            color='white',\n",
    "            fontsize=12,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(facecolor=color, alpha=0.7, edgecolor='none', pad=2)\n",
    "        )\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first few training images with bounding boxes\n",
    "print(\"Sample Training Images with Bounding Boxes:\\n\")\n",
    "\n",
    "images_folder = 'python/train-detector/images'\n",
    "for i in range(min(3, len(tagged_data['files']))):\n",
    "    image_info = tagged_data['files'][i]\n",
    "    image_path = os.path.join(images_folder, image_info['filename'])\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        visualize_bounding_boxes(\n",
    "            image_path,\n",
    "            image_info['tags'],\n",
    "            f\"Training Image {i+1}: {image_info['filename']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload Images with Bounding Box Annotations\n",
    "\n",
    "Upload training images along with their bounding box annotations. Each image can contain multiple objects.\n",
    "\n",
    "**Note**: Azure Custom Vision requires at least 15 images for object detection training, with at least 5 instances of each object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_tagged_images(project_id, tagged_data, images_folder, tags_dict, training_client):\n",
    "    \"\"\"\n",
    "    Upload images with bounding box annotations.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Custom Vision project ID\n",
    "        tagged_data: Dictionary containing image filenames and bounding box data\n",
    "        images_folder: Path to folder containing images\n",
    "        tags_dict: Dictionary mapping tag names to tag objects\n",
    "        training_client: Authenticated training client\n",
    "    \"\"\"\n",
    "    print(\"Uploading images with bounding box annotations...\\n\")\n",
    "    \n",
    "    tagged_images_with_regions = []\n",
    "    \n",
    "    for image_info in tagged_data['files']:\n",
    "        filename = image_info['filename']\n",
    "        image_path = os.path.join(images_folder, filename)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"⚠️  Image not found: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Create regions (bounding boxes) for each object in the image\n",
    "        regions = []\n",
    "        for bbox in image_info['tags']:\n",
    "            tag_name = bbox['tag']\n",
    "            \n",
    "            if tag_name not in tags_dict:\n",
    "                print(f\"⚠️  Unknown tag '{tag_name}' in {filename}\")\n",
    "                continue\n",
    "            \n",
    "            tag_id = tags_dict[tag_name].id\n",
    "            \n",
    "            # Create region with normalized coordinates\n",
    "            region = Region(\n",
    "                tag_id=tag_id,\n",
    "                left=bbox['left'],\n",
    "                top=bbox['top'],\n",
    "                width=bbox['width'],\n",
    "                height=bbox['height']\n",
    "            )\n",
    "            regions.append(region)\n",
    "        \n",
    "        # Read image data\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            image_data = image_file.read()\n",
    "        \n",
    "        # Add to batch\n",
    "        tagged_images_with_regions.append(\n",
    "            ImageFileCreateEntry(\n",
    "                name=filename,\n",
    "                contents=image_data,\n",
    "                regions=regions\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Upload in batches (max 64 images per batch)\n",
    "    batch_size = 64\n",
    "    total_uploaded = 0\n",
    "    \n",
    "    for i in range(0, len(tagged_images_with_regions), batch_size):\n",
    "        batch = tagged_images_with_regions[i:i + batch_size]\n",
    "        \n",
    "        print(f\"Uploading batch {i // batch_size + 1} ({len(batch)} images)...\")\n",
    "        \n",
    "        upload_result = training_client.create_images_from_files(\n",
    "            project_id,\n",
    "            ImageFileCreateBatch(images=batch)\n",
    "        )\n",
    "        \n",
    "        if upload_result.is_batch_successful:\n",
    "            total_uploaded += len(batch)\n",
    "            print(f\"  ✓ Batch uploaded successfully\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  Some images failed to upload\")\n",
    "            for image in upload_result.images:\n",
    "                if image.status != \"OK\":\n",
    "                    print(f\"    - {image.source_url}: {image.status}\")\n",
    "    \n",
    "    print(f\"\\n✓ Total images uploaded: {total_uploaded}\")\n",
    "    return total_uploaded\n",
    "\n",
    "# Upload the tagged images\n",
    "uploaded_count = upload_tagged_images(\n",
    "    project.id,\n",
    "    tagged_data,\n",
    "    images_folder,\n",
    "    tags,\n",
    "    training_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify Uploaded Images\n",
    "\n",
    "Check image counts and ensure sufficient training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed statistics\n",
    "print(\"Training Data Summary:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count images\n",
    "total_images = training_client.get_tagged_image_count(project.id)\n",
    "print(f\"\\nTotal tagged images: {total_images}\")\n",
    "\n",
    "# Count instances per tag\n",
    "print(f\"\\nObject Instances by Type:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for tag_name, tag in tags.items():\n",
    "    tag_info = training_client.get_tag(project.id, tag.id)\n",
    "    print(f\"{tag_name:15} : {tag_info.image_count} instances\")\n",
    "\n",
    "# Check if we have enough data\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "if total_images >= 15:\n",
    "    print(\"✓ Sufficient images for training (minimum 15)\")\n",
    "else:\n",
    "    print(f\"⚠️  Warning: Only {total_images} images. Recommend at least 15 for good results.\")\n",
    "\n",
    "min_instances = min(tag.image_count for tag in [training_client.get_tag(project.id, t.id) for t in tags.values()])\n",
    "if min_instances >= 5:\n",
    "    print(\"✓ Sufficient instances per object type (minimum 5 each)\")\n",
    "else:\n",
    "    print(f\"⚠️  Warning: Some objects have fewer than 5 instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train the Object Detection Model\n",
    "\n",
    "Train the model to detect and locate objects. Training may take longer than classification due to the complexity of learning object locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_detector(project_id, training_client):\n",
    "    \"\"\"\n",
    "    Train the object detection model.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Custom Vision project ID\n",
    "        training_client: Authenticated training client\n",
    "    \n",
    "    Returns:\n",
    "        Completed iteration object\n",
    "    \"\"\"\n",
    "    print(\"Starting object detection model training...\")\n",
    "    print(\"This may take 5-10 minutes or longer. Please wait...\\n\")\n",
    "    \n",
    "    # Start training\n",
    "    iteration = training_client.train_project(project_id)\n",
    "    \n",
    "    # Monitor training progress\n",
    "    while iteration.status not in [\"Completed\", \"Failed\"]:\n",
    "        iteration = training_client.get_iteration(project_id, iteration.id)\n",
    "        print(f\"Training status: {iteration.status}\")\n",
    "        \n",
    "        if iteration.status == \"Failed\":\n",
    "            print(\"❌ Training failed!\")\n",
    "            return None\n",
    "        \n",
    "        time.sleep(10)  # Check every 10 seconds\n",
    "    \n",
    "    print(f\"\\n✓ Model trained successfully!\")\n",
    "    print(f\"  Iteration Name: {iteration.name}\")\n",
    "    print(f\"  Iteration ID: {iteration.id}\")\n",
    "    \n",
    "    return iteration\n",
    "\n",
    "# Train the model\n",
    "iteration = train_detector(project.id, training_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model Performance\n",
    "\n",
    "Check the model's performance metrics. For object detection, we look at:\n",
    "- **Precision**: Of all detected objects, how many were correct?\n",
    "- **Recall**: Of all actual objects, how many did we detect?\n",
    "- **mAP (mean Average Precision)**: Overall detection quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iteration:\n",
    "    # Get performance metrics\n",
    "    performance = training_client.get_iteration_performance(project.id, iteration.id)\n",
    "    \n",
    "    print(\"Object Detection Model Performance:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Precision:          {performance.precision:.2%}\")\n",
    "    print(f\"  Recall:             {performance.recall:.2%}\")\n",
    "    print(f\"  Average Precision:  {performance.average_precision:.2%} (mAP)\")\n",
    "    \n",
    "    print(f\"\\nPer-Object Performance:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Object Type':<15} {'Precision':<12} {'Recall':<12} {'AP':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for tag_perf in performance.per_tag_performance:\n",
    "        print(f\"{tag_perf.name:<15} {tag_perf.precision:<12.2%} \"\n",
    "              f\"{tag_perf.recall:<12.2%} {tag_perf.average_precision:<12.2%}\")\n",
    "    \n",
    "    # Visualize performance\n",
    "    tag_names_perf = [tp.name for tp in performance.per_tag_performance]\n",
    "    precisions = [tp.precision for tp in performance.per_tag_performance]\n",
    "    recalls = [tp.recall for tp in performance.per_tag_performance]\n",
    "    aps = [tp.average_precision for tp in performance.per_tag_performance]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(tag_names_perf))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, precisions, width, label='Precision', color='skyblue')\n",
    "    ax.bar(x, recalls, width, label='Recall', color='lightcoral')\n",
    "    ax.bar(x + width, aps, width, label='Average Precision', color='lightgreen')\n",
    "    \n",
    "    ax.set_xlabel('Object Types')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Object Detection Performance by Object Type')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tag_names_perf)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Publish the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the trained model\n",
    "publish_name = \"FruitObjectDetector\"\n",
    "prediction_resource_id = os.getenv('PredictionResourceId', None)\n",
    "\n",
    "if iteration:\n",
    "    try:\n",
    "        if prediction_resource_id:\n",
    "            training_client.publish_iteration(\n",
    "                project.id,\n",
    "                iteration.id,\n",
    "                publish_name,\n",
    "                prediction_resource_id\n",
    "            )\n",
    "        else:\n",
    "            training_client.publish_iteration(\n",
    "                project.id,\n",
    "                iteration.id,\n",
    "                publish_name\n",
    "            )\n",
    "        \n",
    "        print(f\"✓ Model published successfully as '{publish_name}'\")\n",
    "        print(f\"\\n⚠️  Save this publish name for detection: {publish_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Publishing info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Detect Objects in Test Images\n",
    "\n",
    "Use the trained model to detect objects in new images. The model will return:\n",
    "- Object type (tag)\n",
    "- Bounding box coordinates\n",
    "- Confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image_path, project_id, publish_name, prediction_client, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect objects in an image and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        project_id: Custom Vision project ID\n",
    "        publish_name: Published model name\n",
    "        prediction_client: Authenticated prediction client\n",
    "        confidence_threshold: Minimum confidence to display\n",
    "    \n",
    "    Returns:\n",
    "        Detection results\n",
    "    \"\"\"\n",
    "    # Make prediction\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        results = prediction_client.detect_image(\n",
    "            project_id,\n",
    "            publish_name,\n",
    "            image_file.read()\n",
    "        )\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Define colors\n",
    "    colors = {'apple': 'red', 'banana': 'yellow', 'orange': 'orange'}\n",
    "    \n",
    "    # Print and visualize detections\n",
    "    print(f\"\\nDetections for {os.path.basename(image_path)}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    detected_objects = []\n",
    "    \n",
    "    for prediction in results.predictions:\n",
    "        if prediction.probability >= confidence_threshold:\n",
    "            # Store detection info\n",
    "            detected_objects.append({\n",
    "                'tag': prediction.tag_name,\n",
    "                'confidence': prediction.probability,\n",
    "                'bbox': prediction.bounding_box\n",
    "            })\n",
    "            \n",
    "            # Print detection\n",
    "            print(f\"{prediction.tag_name:15} : {prediction.probability:.2%} confidence\")\n",
    "            print(f\"{'':15}   Location: left={prediction.bounding_box.left:.3f}, \"\n",
    "                  f\"top={prediction.bounding_box.top:.3f}, \"\n",
    "                  f\"width={prediction.bounding_box.width:.3f}, \"\n",
    "                  f\"height={prediction.bounding_box.height:.3f}\")\n",
    "            \n",
    "            # Convert normalized coordinates to pixels\n",
    "            left = prediction.bounding_box.left * img_width\n",
    "            top = prediction.bounding_box.top * img_height\n",
    "            width = prediction.bounding_box.width * img_width\n",
    "            height = prediction.bounding_box.height * img_height\n",
    "            \n",
    "            # Draw bounding box\n",
    "            color = colors.get(prediction.tag_name, 'cyan')\n",
    "            rect = patches.Rectangle(\n",
    "                (left, top), width, height,\n",
    "                linewidth=3, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label with confidence\n",
    "            label = f\"{prediction.tag_name} ({prediction.probability:.0%})\"\n",
    "            ax.text(\n",
    "                left, top - 5,\n",
    "                label,\n",
    "                color='white',\n",
    "                fontsize=11,\n",
    "                fontweight='bold',\n",
    "                bbox=dict(facecolor=color, alpha=0.8, edgecolor='none', pad=3)\n",
    "            )\n",
    "    \n",
    "    if not detected_objects:\n",
    "        print(f\"No objects detected above {confidence_threshold:.0%} confidence threshold\")\n",
    "    \n",
    "    ax.set_title(f\"Object Detection: {os.path.basename(image_path)}\", \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on sample images\n",
    "test_image = 'python/test-detector/produce.jpg'\n",
    "\n",
    "if os.path.exists(test_image):\n",
    "    print(f\"Testing object detection on: {test_image}\")\n",
    "    print(\"=\" * 70)\n",
    "    results = detect_objects(\n",
    "        test_image,\n",
    "        project.id,\n",
    "        publish_name,\n",
    "        prediction_client,\n",
    "        confidence_threshold=0.5\n",
    "    )\n",
    "else:\n",
    "    print(f\"Test image not found: {test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Experiment with Confidence Thresholds\n",
    "\n",
    "The confidence threshold determines which detections to accept. Let's see how different thresholds affect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_thresholds(image_path, project_id, publish_name, prediction_client, thresholds=[0.3, 0.5, 0.7, 0.9]):\n",
    "    \"\"\"\n",
    "    Compare detection results at different confidence thresholds.\n",
    "    \"\"\"\n",
    "    # Get predictions once\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        results = prediction_client.detect_image(\n",
    "            project_id,\n",
    "            publish_name,\n",
    "            image_file.read()\n",
    "        )\n",
    "    \n",
    "    print(f\"Confidence Threshold Analysis for {os.path.basename(image_path)}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Threshold':<12} {'Detections':<12} {'Objects Found'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        detections = [p for p in results.predictions if p.probability >= threshold]\n",
    "        objects_list = ', '.join([f\"{p.tag_name}\" for p in detections[:3]])\n",
    "        if len(detections) > 3:\n",
    "            objects_list += \"...\"\n",
    "        \n",
    "        print(f\"{threshold:<12.1f} {len(detections):<12} {objects_list}\")\n",
    "    \n",
    "    # Visualize all detections with confidence scores\n",
    "    print(f\"\\nAll Detected Objects (any confidence):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for pred in sorted(results.predictions, key=lambda p: p.probability, reverse=True):\n",
    "        print(f\"{pred.tag_name:15} : {pred.probability:.2%}\")\n",
    "\n",
    "if os.path.exists(test_image):\n",
    "    compare_thresholds(test_image, project.id, publish_name, prediction_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Annotated Images\n",
    "\n",
    "Save detection results with bounding boxes drawn on images for documentation or review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detection_image(image_path, detections, output_path, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Save an image with bounding boxes drawn.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Input image path\n",
    "        detections: Detection results from prediction\n",
    "        output_path: Where to save annotated image\n",
    "        confidence_threshold: Minimum confidence to draw\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    img_width, img_height = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Define colors\n",
    "    colors = {'apple': 'red', 'banana': 'yellow', 'orange': 'orange'}\n",
    "    \n",
    "    # Draw each detection\n",
    "    for prediction in detections.predictions:\n",
    "        if prediction.probability >= confidence_threshold:\n",
    "            # Convert coordinates\n",
    "            left = prediction.bounding_box.left * img_width\n",
    "            top = prediction.bounding_box.top * img_height\n",
    "            right = left + (prediction.bounding_box.width * img_width)\n",
    "            bottom = top + (prediction.bounding_box.height * img_height)\n",
    "            \n",
    "            # Draw box\n",
    "            color = colors.get(prediction.tag_name, 'cyan')\n",
    "            line_width = max(3, int(img_width / 200))\n",
    "            \n",
    "            for i in range(line_width):\n",
    "                draw.rectangle(\n",
    "                    [(left - i, top - i), (right + i, bottom + i)],\n",
    "                    outline=color\n",
    "                )\n",
    "            \n",
    "            # Draw label\n",
    "            label = f\"{prediction.tag_name}: {prediction.probability:.0%}\"\n",
    "            draw.text((left, top - 20), label, fill=color)\n",
    "    \n",
    "    # Save\n",
    "    img.save(output_path)\n",
    "    print(f\"✓ Saved annotated image to: {output_path}\")\n",
    "\n",
    "# Save detection results\n",
    "if os.path.exists(test_image):\n",
    "    with open(test_image, 'rb') as f:\n",
    "        results = prediction_client.detect_image(project.id, publish_name, f.read())\n",
    "    \n",
    "    output_image = 'detection_output.jpg'\n",
    "    save_detection_image(test_image, results, output_image, confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Best Practices for Object Detection\n",
    "\n",
    "### Training Data Quality:\n",
    "- **Variety**: Include objects at different scales, angles, and lighting\n",
    "- **Accuracy**: Ensure bounding boxes tightly fit objects\n",
    "- **Quantity**: Minimum 15 images, 50+ recommended for production\n",
    "- **Balance**: Similar number of instances for each object type\n",
    "- **Occlusion**: Include partially hidden objects for robustness\n",
    "\n",
    "### Bounding Box Guidelines:\n",
    "- Tight fit around object (not too loose or tight)\n",
    "- Consistent tagging approach across images\n",
    "- Include small objects if they're important\n",
    "- Avoid overlapping boxes when possible\n",
    "\n",
    "### Model Optimization:\n",
    "- **Threshold tuning**: Balance false positives vs false negatives\n",
    "- **IoU threshold**: Adjust for overlapping object handling\n",
    "- **Retrain regularly**: Add misdetected images to training set\n",
    "- **Domain selection**: Use appropriate domain for your scenario\n",
    "\n",
    "### Production Deployment:\n",
    "- Implement confidence threshold filtering\n",
    "- Handle cases with no detections gracefully\n",
    "- Monitor detection frequencies and confidence distributions\n",
    "- Log low-confidence detections for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summary\n",
    "\n",
    "### What You've Learned:\n",
    "✓ Understood difference between classification and object detection  \n",
    "✓ Created an object detection project  \n",
    "✓ Uploaded images with bounding box annotations  \n",
    "✓ Trained an object detection model  \n",
    "✓ Evaluated model performance metrics  \n",
    "✓ Detected objects in test images with confidence scores  \n",
    "✓ Visualized detections with bounding boxes  \n",
    "✓ Experimented with confidence thresholds  \n",
    "\n",
    "### Key Concepts:\n",
    "- **Bounding boxes**: Normalized coordinates (0-1) define object locations\n",
    "- **Confidence threshold**: Filters detections by minimum probability\n",
    "- **mAP**: Mean average precision measures overall detection quality\n",
    "- **Precision vs Recall**: Tradeoff between false positives and false negatives\n",
    "\n",
    "### Next Steps:\n",
    "- Explore the **Advanced Object Detection** notebook for:\n",
    "  - IoU (Intersection over Union) metrics\n",
    "  - Non-maximum suppression\n",
    "  - Multi-object detection strategies\n",
    "  - Real-time detection optimization\n",
    "\n",
    "### Important Information:\n",
    "```python\n",
    "Project ID: [YOUR_PROJECT_ID]\n",
    "Published Model: [YOUR_MODEL_NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the project\n",
    "# training_client.delete_project(project.id)\n",
    "# print(\"Project deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Azure Custom Vision Object Detection Documentation](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/get-started-build-detector)\n",
    "- [Object Detection Best Practices](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/suggested-tags)\n",
    "- [Understanding Object Detection Metrics](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/probability-threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
