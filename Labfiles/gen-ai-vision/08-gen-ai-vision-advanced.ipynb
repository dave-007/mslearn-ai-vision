{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08: Advanced Generative AI with Vision\n",
    "\n",
    "## Overview\n",
    "This advanced notebook explores sophisticated multimodal AI techniques using Azure OpenAI's GPT-4 Vision. You'll learn advanced prompt engineering, complex image reasoning, batch processing, and production-ready patterns.\n",
    "\n",
    "## Advanced Topics Covered\n",
    "- Complex image reasoning and analysis\n",
    "- Image comparison and visual question answering\n",
    "- Chain-of-thought reasoning with images\n",
    "- Few-shot learning for vision tasks\n",
    "- Custom image captioning styles\n",
    "- Content moderation and safety\n",
    "- Batch image analysis\n",
    "- Advanced prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-projects azure-identity python-dotenv pillow matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import UserMessage, ImageContentItem, ImageUrl, TextContentItem\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load configuration\n",
    "load_dotenv('python/.env')\n",
    "project_endpoint = os.getenv(\"PROJECT_CONNECTION\")\n",
    "model_deployment = os.getenv(\"MODEL_DEPLOYMENT\")\n",
    "\n",
    "# Initialize client\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    conn_str=project_endpoint,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "chat_client = project_client.inference.get_chat_completions_client()\n",
    "\n",
    "print(\"‚úì Environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Complex Image Reasoning\n",
    "\n",
    "GPT-4 Vision can perform complex reasoning tasks including counting, spatial relationships, and logical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_reasoning(image_path, prompt, show_reasoning=True):\n",
    "    \"\"\"Analyze image with chain-of-thought reasoning.\"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    reasoning_prompt = f\"\"\"{prompt}\n",
    "    \n",
    "{'Please explain your reasoning step-by-step before giving your final answer.' if show_reasoning else ''}\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=reasoning_prompt),\n",
    "            ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "        ]\n",
    "    )]\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Complex reasoning about produce\n",
    "print(\"üß† Complex Reasoning Example:\\n\")\n",
    "result = analyze_with_reasoning(\n",
    "    \"mango.jpeg\",\n",
    "    \"Based on the visual characteristics, estimate how many days until this fruit reaches peak ripeness.\",\n",
    "    show_reasoning=True\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Comparison and Analysis\n",
    "\n",
    "Compare multiple images to identify differences, similarities, or make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image_paths: List[str], comparison_prompt: str):\n",
    "    \"\"\"Compare multiple images with a custom prompt.\"\"\"\n",
    "    content = [TextContentItem(text=comparison_prompt)]\n",
    "    \n",
    "    # Add all images to the message\n",
    "    for i, image_path in enumerate(image_paths, 1):\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        content.append(ImageContentItem(\n",
    "            image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\")\n",
    "        ))\n",
    "    \n",
    "    messages = [UserMessage(content=content)]\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Compare mango and orange\n",
    "print(\"üîç Comparing Images:\\n\")\n",
    "comparison = compare_images(\n",
    "    [\"mango.jpeg\", \"orange.jpeg\"],\n",
    "    \"\"\"I've provided two fruit images. Please:\n",
    "    1. Identify each fruit\n",
    "    2. Compare their nutritional profiles\n",
    "    3. Compare their shelf life\n",
    "    4. Recommend which one is better for a smoothie and why\n",
    "    \"\"\"\n",
    ")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Learning with Vision\n",
    "\n",
    "Teach the model specific patterns or styles by providing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_vision_learning(examples: List[Dict], test_image: str):\n",
    "    \"\"\"Use few-shot learning to teach the model a specific task.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a produce quality inspector. Learn from the examples provided.\"}\n",
    "    ]\n",
    "    \n",
    "    # Add example pairs (image + expected output)\n",
    "    for example in examples:\n",
    "        with open(example['image'], \"rb\") as img_file:\n",
    "            base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        \n",
    "        messages.append(UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"Analyze this produce:\"),\n",
    "                ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "            ]\n",
    "        ))\n",
    "        messages.append({\"role\": \"assistant\", \"content\": example['output']})\n",
    "    \n",
    "    # Now test on new image\n",
    "    with open(test_image, \"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    messages.append(UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=\"Analyze this produce:\"),\n",
    "            ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "        ]\n",
    "    ))\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Teach a specific quality grading format\n",
    "print(\"üìö Few-Shot Learning Example:\\n\")\n",
    "examples = [\n",
    "    {\n",
    "        'image': 'mango.jpeg',\n",
    "        'output': '''Quality Grade: A\n",
    "Ripeness: 85%\n",
    "Shelf Life: 3-4 days\n",
    "Visual Quality: Excellent color, no blemishes\n",
    "Recommendation: Ready for immediate sale'''\n",
    "    }\n",
    "]\n",
    "\n",
    "result = few_shot_vision_learning(examples, \"orange.jpeg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Image Captioning Styles\n",
    "\n",
    "Generate image captions in different styles or formats for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custom_caption(image_path, style):\n",
    "    \"\"\"Generate image captions in different styles.\"\"\"\n",
    "    \n",
    "    style_prompts = {\n",
    "        'technical': 'Provide a technical, detailed description suitable for a product catalog.',\n",
    "        'poetic': 'Write a poetic, artistic description of the image.',\n",
    "        'marketing': 'Write compelling marketing copy to sell this product.',\n",
    "        'scientific': 'Describe from a botanical/scientific perspective.',\n",
    "        'social_media': 'Write an engaging social media post with emojis.',\n",
    "        'accessibility': 'Write an accessibility-friendly alt text description.'\n",
    "    }\n",
    "    \n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    prompt = f\"Describe this image. Style: {style_prompts.get(style, style)}\"\n",
    "    \n",
    "    messages = [UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=prompt),\n",
    "            ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "        ]\n",
    "    )]\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate captions in multiple styles\n",
    "print(\"üé® Custom Caption Styles:\\n\")\n",
    "styles = ['technical', 'poetic', 'marketing', 'social_media']\n",
    "\n",
    "for style in styles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Style: {style.upper()}\")\n",
    "    print('='*60)\n",
    "    caption = generate_custom_caption(\"mango.jpeg\", style)\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Content Moderation with Vision\n",
    "\n",
    "Use GPT-4 Vision to analyze images for quality, safety, and appropriateness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate_image_content(image_path, criteria):\n",
    "    \"\"\"Analyze image against specific content criteria.\"\"\"\n",
    "    \n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this image for the following criteria:\n",
    "    \n",
    "    {chr(10).join(f'- {criterion}' for criterion in criteria)}\n",
    "    \n",
    "    Provide a JSON response with:\n",
    "    - overall_status: \"approved\" or \"rejected\"\n",
    "    - confidence: 0-100\n",
    "    - issues: list of any concerns\n",
    "    - recommendations: list of suggestions\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=prompt),\n",
    "            ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "        ]\n",
    "    )]\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Product quality check\n",
    "print(\"üõ°Ô∏è Content Moderation Example:\\n\")\n",
    "quality_criteria = [\n",
    "    \"Product is clearly visible and in focus\",\n",
    "    \"No visible damage or defects\",\n",
    "    \"Appropriate for retail display\",\n",
    "    \"Professional product photography standards\",\n",
    "    \"Accurate color representation\"\n",
    "]\n",
    "\n",
    "moderation_result = moderate_image_content(\"mango.jpeg\", quality_criteria)\n",
    "print(moderation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Image Analysis\n",
    "\n",
    "Efficiently process multiple images with structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analyze_images(image_paths, analysis_template):\n",
    "    \"\"\"Analyze multiple images with a consistent template.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        print(f\"Analyzing: {image_path}...\")\n",
    "        \n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        \n",
    "        messages = [UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=analysis_template),\n",
    "                ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "            ]\n",
    "        )]\n",
    "        \n",
    "        response = chat_client.complete(\n",
    "            model=model_deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'image': image_path,\n",
    "            'analysis': response.choices[0].message.content\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Batch process available images\n",
    "print(\"üìä Batch Analysis:\\n\")\n",
    "images_to_analyze = [\"mango.jpeg\", \"orange.jpeg\"]\n",
    "\n",
    "template = \"\"\"Provide a structured analysis:\n",
    "1. Fruit Type:\n",
    "2. Estimated Weight:\n",
    "3. Ripeness (1-10):\n",
    "4. Price Suggestion:\n",
    "5. Marketing Angle:\n",
    "\"\"\"\n",
    "\n",
    "batch_results = batch_analyze_images(images_to_analyze, template)\n",
    "\n",
    "for result in batch_results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Image: {result['image']}\")\n",
    "    print('='*60)\n",
    "    print(result['analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Prompt Engineering Techniques\n",
    "\n",
    "Explore sophisticated prompting strategies for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_analysis(image_path, structure):\n",
    "    \"\"\"Get structured, parseable output from vision analysis.\"\"\"\n",
    "    \n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this image and provide a response in the following JSON structure:\n",
    "    \n",
    "    {json.dumps(structure, indent=2)}\n",
    "    \n",
    "    Provide ONLY valid JSON, no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=prompt),\n",
    "            ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "        ]\n",
    "    )]\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=600\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Get structured product data\n",
    "print(\"üèóÔ∏è Structured Analysis Example:\\n\")\n",
    "structure_template = {\n",
    "    \"product_name\": \"string\",\n",
    "    \"category\": \"string\",\n",
    "    \"color\": [\"primary_color\", \"secondary_color\"],\n",
    "    \"quality_score\": \"1-10\",\n",
    "    \"attributes\": {\n",
    "        \"size\": \"small/medium/large\",\n",
    "        \"ripeness\": \"percentage\",\n",
    "        \"condition\": \"description\"\n",
    "    },\n",
    "    \"recommended_use\": \"string\",\n",
    "    \"storage_instructions\": \"string\"\n",
    "}\n",
    "\n",
    "structured_result = structured_analysis(\"mango.jpeg\", structure_template)\n",
    "print(structured_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visual Question Answering with Confidence Scores\n",
    "\n",
    "Get answers with confidence levels for decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_with_confidence(image_path, question):\n",
    "    \"\"\"Visual Question Answering with confidence scoring.\"\"\"\n",
    "    \n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    prompt = f\"\"\"{question}\n",
    "    \n",
    "    Provide your answer in this format:\n",
    "    Answer: [your answer]\n",
    "    Confidence: [0-100]%\n",
    "    Reasoning: [brief explanation]\n",
    "    Alternative: [if applicable, alternative interpretation]\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=prompt),\n",
    "            ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "        ]\n",
    "    )]\n",
    "    \n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example questions\n",
    "print(\"‚ùì VQA with Confidence:\\n\")\n",
    "questions = [\n",
    "    \"Is this fruit organic?\",\n",
    "    \"What is the estimated ripening date?\",\n",
    "    \"Would this fruit be suitable for making juice?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(\"-\" * 60)\n",
    "    answer = vqa_with_confidence(\"mango.jpeg\", q)\n",
    "    print(answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_vision_analysis(image_path, prompt, max_retries=3):\n",
    "    \"\"\"Robust vision analysis with error handling and retries.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Validate image\n",
    "            if not os.path.exists(image_path):\n",
    "                return {\"error\": \"Image file not found\", \"status\": \"failed\"}\n",
    "            \n",
    "            # Check file size (GPT-4 Vision has limits)\n",
    "            file_size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
    "            if file_size_mb > 20:\n",
    "                return {\"error\": \"Image too large (>20MB)\", \"status\": \"failed\"}\n",
    "            \n",
    "            # Process image\n",
    "            with open(image_path, \"rb\") as img_file:\n",
    "                base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            \n",
    "            messages = [UserMessage(\n",
    "                content=[\n",
    "                    TextContentItem(text=prompt),\n",
    "                    ImageContentItem(image_url=ImageUrl(url=f\"data:image/jpeg;base64,{base64_image}\"))\n",
    "                ]\n",
    "            )]\n",
    "            \n",
    "            response = chat_client.complete(\n",
    "                model=model_deployment,\n",
    "                messages=messages,\n",
    "                max_tokens=500,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"result\": response.choices[0].message.content,\n",
    "                \"status\": \"success\",\n",
    "                \"attempt\": attempt + 1\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"error\": str(e),\n",
    "                    \"status\": \"failed\",\n",
    "                    \"attempts\": max_retries\n",
    "                }\n",
    "            print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "    \n",
    "    return {\"error\": \"Max retries exceeded\", \"status\": \"failed\"}\n",
    "\n",
    "# Test error handling\n",
    "print(\"üõ†Ô∏è Testing Robust Analysis:\\n\")\n",
    "result = robust_vision_analysis(\"mango.jpeg\", \"Describe this image briefly.\")\n",
    "print(f\"Status: {result['status']}\")\n",
    "if result['status'] == 'success':\n",
    "    print(f\"Result: {result['result']}\")\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this advanced lab, you explored:\n",
    "\n",
    "‚úÖ **Complex reasoning** - Chain-of-thought and logical analysis  \n",
    "‚úÖ **Image comparison** - Multi-image analysis and recommendations  \n",
    "‚úÖ **Few-shot learning** - Teaching custom patterns to the model  \n",
    "‚úÖ **Custom captioning** - Style-specific descriptions  \n",
    "‚úÖ **Content moderation** - Quality and safety checking  \n",
    "‚úÖ **Batch processing** - Efficient multi-image workflows  \n",
    "‚úÖ **Structured output** - JSON and parseable responses  \n",
    "‚úÖ **Confidence scoring** - Decision-making support  \n",
    "‚úÖ **Error handling** - Production-ready patterns  \n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Be specific in prompts** - Detailed instructions yield better results\n",
    "2. **Use structured output** - Request JSON for parseable responses\n",
    "3. **Implement retries** - Handle transient failures gracefully\n",
    "4. **Validate images** - Check size and format before processing\n",
    "5. **Use few-shot examples** - Guide the model with demonstrations\n",
    "6. **Request confidence scores** - Make informed decisions\n",
    "7. **Batch efficiently** - Process multiple images systematically\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "- **Rate limiting**: Implement throttling for API calls\n",
    "- **Caching**: Cache results for repeated queries\n",
    "- **Cost optimization**: Monitor token usage\n",
    "- **Security**: Validate and sanitize image inputs\n",
    "- **Monitoring**: Track success rates and errors\n",
    "- **Fallbacks**: Have backup strategies for failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
