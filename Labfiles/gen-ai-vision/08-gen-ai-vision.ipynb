{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08: Generative AI with Vision\n",
    "\n",
    "## Overview\n",
    "In this lab, you'll learn how to use Azure OpenAI's GPT-4 Vision model to analyze and understand images. You'll build an interactive application that can answer questions about images, similar to a smart grocery store assistant that helps identify and provide information about produce.\n",
    "\n",
    "## Learning Objectives\n",
    "- Connect to Azure OpenAI GPT-4 Vision service\n",
    "- Analyze images with natural language queries\n",
    "- Build multi-turn conversations with image context\n",
    "- Understand prompt engineering for vision tasks\n",
    "\n",
    "## Prerequisites\n",
    "- Azure OpenAI resource with GPT-4 Vision deployment\n",
    "- API credentials configured in `.env` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "First, let's install the required packages and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install azure-ai-projects azure-identity python-dotenv pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import UserMessage, ImageContentItem, ImageUrl, TextContentItem\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"‚úì Packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration\n",
    "\n",
    "Load the Azure OpenAI credentials from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the python subfolder\n",
    "load_dotenv('python/.env')\n",
    "\n",
    "project_endpoint = os.getenv(\"PROJECT_CONNECTION\")\n",
    "model_deployment = os.getenv(\"MODEL_DEPLOYMENT\")\n",
    "\n",
    "if not project_endpoint or not model_deployment:\n",
    "    print(\"‚ö†Ô∏è  Please configure PROJECT_CONNECTION and MODEL_DEPLOYMENT in python/.env file\")\n",
    "else:\n",
    "    print(f\"‚úì Configuration loaded\")\n",
    "    print(f\"  Endpoint: {project_endpoint[:50]}...\")\n",
    "    print(f\"  Model Deployment: {model_deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Azure OpenAI Client\n",
    "\n",
    "Create a connection to the Azure OpenAI service and get a chat client for GPT-4 Vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the project client\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    conn_str=project_endpoint,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Get a chat client\n",
    "chat_client = project_client.inference.get_chat_completions_client()\n",
    "\n",
    "print(\"‚úì Azure OpenAI client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions\n",
    "\n",
    "Create utility functions to encode images and display them in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Encode image file to base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def display_image(image_path, width=400):\n",
    "    \"\"\"Display an image in the notebook.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    display(img.resize((width, int(img.height * width / img.width))))\n",
    "\n",
    "def get_vision_response(image_path, user_prompt, system_prompt=None):\n",
    "    \"\"\"Get a response from GPT-4 Vision for an image and prompt.\"\"\"\n",
    "    \n",
    "    # Encode image to base64\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # Create user message with image and text\n",
    "    messages.append(UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=user_prompt),\n",
    "            ImageContentItem(image_url=ImageUrl(url=image_url))\n",
    "        ]\n",
    "    ))\n",
    "    \n",
    "    # Get response\n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Your First Image\n",
    "\n",
    "Let's analyze an image of a mango and ask questions about it. This demonstrates the basic image understanding capabilities of GPT-4 Vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the mango image\n",
    "image_path = \"mango.jpeg\"\n",
    "\n",
    "# Display the image\n",
    "print(\"üñºÔ∏è  Analyzing image:\")\n",
    "display_image(image_path)\n",
    "\n",
    "# Define system message\n",
    "system_message = \"You are an AI assistant in a grocery store that sells fruit. You provide detailed answers to questions about produce.\"\n",
    "\n",
    "# Ask a question about the image\n",
    "question = \"What fruit is this?\"\n",
    "print(f\"\\n‚ùì Question: {question}\")\n",
    "print(\"\\nü§ñ Response:\")\n",
    "\n",
    "response = get_vision_response(image_path, question, system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Ask More Detailed Questions\n",
    "\n",
    "GPT-4 Vision can answer more complex questions about the image, such as ripeness, nutritional information, and usage suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question about ripeness\n",
    "question = \"Is this mango ripe? How can you tell?\"\n",
    "print(f\"‚ùì Question: {question}\\n\")\n",
    "\n",
    "response = get_vision_response(image_path, question, system_message)\n",
    "print(f\"ü§ñ Response:\\n{response}\\n\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question about nutritional value\n",
    "question = \"What are the nutritional benefits of this fruit?\"\n",
    "print(f\"‚ùì Question: {question}\\n\")\n",
    "\n",
    "response = get_vision_response(image_path, question, system_message)\n",
    "print(f\"ü§ñ Response:\\n{response}\\n\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question about usage\n",
    "question = \"What are some ways to use this fruit in cooking or recipes?\"\n",
    "print(f\"‚ùì Question: {question}\\n\")\n",
    "\n",
    "response = get_vision_response(image_path, question, system_message)\n",
    "print(f\"ü§ñ Response:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Another Image (Orange)\n",
    "\n",
    "Let's try with a different fruit to see how the model adapts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the orange image\n",
    "orange_image = \"orange.jpeg\"\n",
    "\n",
    "# Display the image\n",
    "print(\"üñºÔ∏è  Analyzing image:\")\n",
    "display_image(orange_image)\n",
    "\n",
    "# Ask questions\n",
    "question = \"What type of fruit is this and what are its characteristics?\"\n",
    "print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "\n",
    "response = get_vision_response(orange_image, question, system_message)\n",
    "print(f\"ü§ñ Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Multi-Turn Conversation (Advanced)\n",
    "\n",
    "For multi-turn conversations where you want to maintain context across multiple questions about the same image, you can build a conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_turn_conversation(image_path, questions, system_prompt=None):\n",
    "    \"\"\"Have a multi-turn conversation about an image.\"\"\"\n",
    "    \n",
    "    # Encode image once\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # Add first message with image\n",
    "    messages.append(UserMessage(\n",
    "        content=[\n",
    "            TextContentItem(text=questions[0]),\n",
    "            ImageContentItem(image_url=ImageUrl(url=image_url))\n",
    "        ]\n",
    "    ))\n",
    "    \n",
    "    # Get first response\n",
    "    response = chat_client.complete(\n",
    "        model=model_deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    print(f\"‚ùì Q1: {questions[0]}\")\n",
    "    print(f\"ü§ñ A1: {response.choices[0].message.content}\\n\")\n",
    "    \n",
    "    # Add assistant's response to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    \n",
    "    # Continue conversation with follow-up questions\n",
    "    for i, question in enumerate(questions[1:], start=2):\n",
    "        messages.append({\"role\": \"user\", \"content\": question})\n",
    "        \n",
    "        response = chat_client.complete(\n",
    "            model=model_deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        print(f\"‚ùì Q{i}: {question}\")\n",
    "        print(f\"ü§ñ A{i}: {response.choices[0].message.content}\\n\")\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "print(\"‚úì Multi-turn conversation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mystery fruit image\n",
    "mystery_image = \"python/mystery-fruit.jpeg\"\n",
    "print(\"üñºÔ∏è  Mystery Fruit:\")\n",
    "display_image(mystery_image)\n",
    "print(\"\\nüí¨ Starting conversation...\\n\")\n",
    "\n",
    "# Define a series of related questions\n",
    "conversation_questions = [\n",
    "    \"What fruit is in this image?\",\n",
    "    \"How can I tell when it's ripe?\",\n",
    "    \"What's the best way to store it?\",\n",
    "    \"Can you suggest a simple recipe using it?\"\n",
    "]\n",
    "\n",
    "# Have the conversation\n",
    "multi_turn_conversation(mystery_image, conversation_questions, system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Interactive Q&A Session\n",
    "\n",
    "Create an interactive widget where you can ask your own questions about an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_qa(image_path):\n",
    "    \"\"\"Interactive Q&A session for an image.\"\"\"\n",
    "    print(\"üñºÔ∏è  Current Image:\")\n",
    "    display_image(image_path)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üí° Ask questions about the image (type 'quit' to exit)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    system_prompt = \"You are an AI assistant in a grocery store that sells fruit. You provide detailed answers to questions about produce.\"\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n‚ùì Your question: \").strip()\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            print(\"\\nüëã Thank you for using the image Q&A assistant!\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            print(\"‚ö†Ô∏è  Please enter a question.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nü§î Analyzing...\")\n",
    "        response = get_vision_response(image_path, question, system_prompt)\n",
    "        print(f\"\\nü§ñ Response:\\n{response}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "# Uncomment the line below to start an interactive session\n",
    "# interactive_qa(\"mango.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Experiment with Your Own Images\n",
    "\n",
    "Try analyzing your own images! Place an image file in the current directory and update the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image and specify the path\n",
    "# your_image = \"path/to/your/image.jpg\"\n",
    "# display_image(your_image)\n",
    "# \n",
    "# your_question = \"Describe what you see in this image.\"\n",
    "# response = get_vision_response(your_image, your_question)\n",
    "# print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "‚úÖ **Connect to Azure OpenAI GPT-4 Vision** - Initialize and authenticate with the service  \n",
    "‚úÖ **Analyze images** - Ask questions and get detailed responses about image content  \n",
    "‚úÖ **Build multi-turn conversations** - Maintain context across multiple questions  \n",
    "‚úÖ **Create interactive applications** - Build practical Q&A systems with vision capabilities  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- GPT-4 Vision can understand and analyze images with natural language\n",
    "- Images must be encoded as base64 or provided as URLs\n",
    "- System prompts help guide the model's behavior and responses\n",
    "- Multi-turn conversations allow for deeper, context-aware interactions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore the **Advanced Notebook** (08-gen-ai-vision-advanced.ipynb) for more complex vision AI scenarios\n",
    "- Try different types of images (products, scenes, documents)\n",
    "- Experiment with different system prompts to customize behavior\n",
    "- Combine vision with other AI capabilities for richer applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
