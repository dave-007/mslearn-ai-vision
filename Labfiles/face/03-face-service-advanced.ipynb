{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Face Service - Advanced Topics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers advanced face analysis capabilities provided by the Azure Face API. Building on the basic face detection skills, you'll explore:\n",
    "\n",
    "- **Face Comparison & Verification**: Determine if two faces belong to the same person\n",
    "- **Similar Face Finding**: Find faces similar to a target face\n",
    "- **Face Grouping**: Automatically group similar faces together\n",
    "- **Face Landmarks**: Analyze detailed facial feature points (27 landmarks)\n",
    "- **Recognition Best Practices**: Tips for building robust face recognition systems\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of the basic Face Service lab (03-face-service.ipynb)\n",
    "- Azure subscription with Azure AI Services resource\n",
    "- Understanding of basic face detection concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Configure Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install azure-cognitiveservices-vision-face python-dotenv matplotlib pillow numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from azure.cognitiveservices.vision.face.models import (\n",
    "    FaceAttributeType, \n",
    "    TrainingStatusType,\n",
    "    QualityForRecognition\n",
    ")\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('python/face-api/.env')\n",
    "cog_endpoint = os.getenv('AI_SERVICE_ENDPOINT')\n",
    "cog_key = os.getenv('AI_SERVICE_KEY')\n",
    "\n",
    "# Create Face client\n",
    "credentials = CognitiveServicesCredentials(cog_key)\n",
    "face_client = FaceClient(cog_endpoint, credentials)\n",
    "\n",
    "print('‚úì Face client configured successfully!')\n",
    "print(f'‚úì Endpoint: {cog_endpoint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Face Verification\n",
    "\n",
    "### What is Face Verification?\n",
    "\n",
    "Face verification answers the question: \"Are these two faces from the same person?\" It's a 1:1 matching operation commonly used in:\n",
    "- Identity verification systems\n",
    "- Access control\n",
    "- Authentication processes\n",
    "\n",
    "The API returns:\n",
    "- **isIdentical**: Boolean indicating if faces match\n",
    "- **confidence**: Confidence score (0-1) for the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_faces(image_file1, image_file2):\n",
    "    \"\"\"\n",
    "    Verify if two faces belong to the same person.\n",
    "    \n",
    "    Args:\n",
    "        image_file1: Path to first image\n",
    "        image_file2: Path to second image\n",
    "    \n",
    "    Returns:\n",
    "        Verification result with confidence score\n",
    "    \"\"\"\n",
    "    print(f'\\nVerifying faces...')\n",
    "    print(f'Image 1: {image_file1}')\n",
    "    print(f'Image 2: {image_file2}')\n",
    "    \n",
    "    # Detect face in first image\n",
    "    with open(image_file1, 'rb') as image_stream:\n",
    "        faces1 = face_client.face.detect_with_stream(\n",
    "            image_stream,\n",
    "            detection_model='detection_03',\n",
    "            recognition_model='recognition_04',\n",
    "            return_face_attributes=[FaceAttributeType.quality_for_recognition]\n",
    "        )\n",
    "    \n",
    "    if not faces1:\n",
    "        print('‚ö†Ô∏è  No face detected in first image')\n",
    "        return None\n",
    "    \n",
    "    face1_id = faces1[0].face_id\n",
    "    quality1 = faces1[0].face_attributes.quality_for_recognition\n",
    "    print(f'‚úì Face 1 detected (Quality: {quality1})')\n",
    "    \n",
    "    # Detect face in second image\n",
    "    with open(image_file2, 'rb') as image_stream:\n",
    "        faces2 = face_client.face.detect_with_stream(\n",
    "            image_stream,\n",
    "            detection_model='detection_03',\n",
    "            recognition_model='recognition_04',\n",
    "            return_face_attributes=[FaceAttributeType.quality_for_recognition]\n",
    "        )\n",
    "    \n",
    "    if not faces2:\n",
    "        print('‚ö†Ô∏è  No face detected in second image')\n",
    "        return None\n",
    "    \n",
    "    face2_id = faces2[0].face_id\n",
    "    quality2 = faces2[0].face_attributes.quality_for_recognition\n",
    "    print(f'‚úì Face 2 detected (Quality: {quality2})')\n",
    "    \n",
    "    # Verify if faces match\n",
    "    verify_result = face_client.face.verify_face_to_face(face1_id, face2_id)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print('VERIFICATION RESULTS')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'Match: {\"‚úì YES\" if verify_result.is_identical else \"‚úó NO\"}')\n",
    "    print(f'Confidence: {verify_result.confidence:.4f}')\n",
    "    \n",
    "    # Interpretation\n",
    "    if verify_result.is_identical:\n",
    "        print(f'\\nüéØ These faces likely belong to the SAME person')\n",
    "    else:\n",
    "        print(f'\\n‚ùå These faces likely belong to DIFFERENT people')\n",
    "    \n",
    "    # Display images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    img1 = Image.open(image_file1)\n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title('Image 1')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    img2 = Image.open(image_file2)\n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title('Image 2')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    match_text = f'Match: {\"YES\" if verify_result.is_identical else \"NO\"} (Confidence: {verify_result.confidence:.2%})'\n",
    "    fig.suptitle(match_text, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return verify_result\n",
    "\n",
    "# Example: Verify two faces\n",
    "# Note: You'll need multiple images to test this. For now, we'll use the same image twice\n",
    "image1 = 'python/face-api/images/face1.jpg'\n",
    "image2 = 'python/face-api/images/face2.jpg'\n",
    "\n",
    "if os.path.exists(image1) and os.path.exists(image2):\n",
    "    result = verify_faces(image1, image2)\n",
    "else:\n",
    "    print('Images not found. Please ensure face1.jpg and face2.jpg exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Verification Thresholds\n",
    "\n",
    "The confidence score helps determine match accuracy:\n",
    "- **High confidence (>0.7)**: Strong match\n",
    "- **Medium confidence (0.5-0.7)**: Possible match, needs review\n",
    "- **Low confidence (<0.5)**: Different people\n",
    "\n",
    "The threshold can be adjusted based on your security requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Finding Similar Faces\n",
    "\n",
    "### What is Face Finding?\n",
    "\n",
    "Face finding is a 1:N operation that searches for faces similar to a target face. Use cases include:\n",
    "- Photo organization and tagging\n",
    "- Finding duplicate or similar photos\n",
    "- Person search in image collections\n",
    "\n",
    "The API can operate in two modes:\n",
    "- **matchPerson**: Find faces of the same person\n",
    "- **matchFace**: Find faces with similar appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_faces(target_image, candidate_images):\n",
    "    \"\"\"\n",
    "    Find faces similar to a target face from a list of candidates.\n",
    "    \n",
    "    Args:\n",
    "        target_image: Path to target image\n",
    "        candidate_images: List of candidate image paths\n",
    "    \"\"\"\n",
    "    print(f'Finding similar faces to: {target_image}')\n",
    "    \n",
    "    # Detect target face\n",
    "    with open(target_image, 'rb') as image_stream:\n",
    "        target_faces = face_client.face.detect_with_stream(\n",
    "            image_stream,\n",
    "            detection_model='detection_03',\n",
    "            recognition_model='recognition_04'\n",
    "        )\n",
    "    \n",
    "    if not target_faces:\n",
    "        print('‚ö†Ô∏è  No face detected in target image')\n",
    "        return\n",
    "    \n",
    "    target_face_id = target_faces[0].face_id\n",
    "    print(f'‚úì Target face detected')\n",
    "    \n",
    "    # Detect candidate faces\n",
    "    candidate_face_ids = []\n",
    "    candidate_info = []\n",
    "    \n",
    "    print(f'\\nDetecting candidate faces...')\n",
    "    for img_path in candidate_images:\n",
    "        if os.path.exists(img_path):\n",
    "            with open(img_path, 'rb') as image_stream:\n",
    "                faces = face_client.face.detect_with_stream(\n",
    "                    image_stream,\n",
    "                    detection_model='detection_03',\n",
    "                    recognition_model='recognition_04'\n",
    "                )\n",
    "            \n",
    "            if faces:\n",
    "                candidate_face_ids.append(faces[0].face_id)\n",
    "                candidate_info.append({'path': img_path, 'face_id': faces[0].face_id})\n",
    "                print(f'  ‚úì {img_path}')\n",
    "    \n",
    "    if not candidate_face_ids:\n",
    "        print('‚ö†Ô∏è  No candidate faces detected')\n",
    "        return\n",
    "    \n",
    "    # Find similar faces\n",
    "    print(f'\\nSearching for similar faces...')\n",
    "    similar_faces = face_client.face.find_similar(\n",
    "        face_id=target_face_id,\n",
    "        face_ids=candidate_face_ids,\n",
    "        mode='matchPerson'\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Found {len(similar_faces)} similar face(s)')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    for i, similar in enumerate(similar_faces, 1):\n",
    "        # Find matching candidate\n",
    "        matching_candidate = next((c for c in candidate_info if c['face_id'] == similar.face_id), None)\n",
    "        if matching_candidate:\n",
    "            print(f'\\n{i}. {matching_candidate[\"path\"]}')\n",
    "            print(f'   Confidence: {similar.confidence:.4f}')\n",
    "    \n",
    "    # Visualize results\n",
    "    if similar_faces:\n",
    "        num_results = min(len(similar_faces) + 1, 4)\n",
    "        fig, axes = plt.subplots(1, num_results, figsize=(5*num_results, 5))\n",
    "        \n",
    "        if num_results == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Show target\n",
    "        img = Image.open(target_image)\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title('TARGET', fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Show similar faces\n",
    "        for i, similar in enumerate(similar_faces[:3], 1):\n",
    "            matching_candidate = next((c for c in candidate_info if c['face_id'] == similar.face_id), None)\n",
    "            if matching_candidate:\n",
    "                img = Image.open(matching_candidate['path'])\n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f'Match {i}\\n{similar.confidence:.2%}', fontweight='bold')\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example: Find similar faces\n",
    "target = 'python/face-api/images/face1.jpg'\n",
    "candidates = [\n",
    "    'python/face-api/images/face2.jpg',\n",
    "    'python/face-api/images/faces.jpg'\n",
    "]\n",
    "\n",
    "if os.path.exists(target):\n",
    "    existing_candidates = [c for c in candidates if os.path.exists(c)]\n",
    "    if existing_candidates:\n",
    "        find_similar_faces(target, existing_candidates)\n",
    "    else:\n",
    "        print('No candidate images found')\n",
    "else:\n",
    "    print('Target image not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Face Grouping\n",
    "\n",
    "### What is Face Grouping?\n",
    "\n",
    "Face grouping automatically organizes faces into groups based on similarity. The algorithm:\n",
    "- Groups faces that likely belong to the same person\n",
    "- Places uncertain faces into a \"messyGroup\"\n",
    "- Useful for organizing large photo collections\n",
    "\n",
    "This is an unsupervised clustering operation - no training required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_faces(image_files):\n",
    "    \"\"\"\n",
    "    Group similar faces from multiple images.\n",
    "    \n",
    "    Args:\n",
    "        image_files: List of image paths\n",
    "    \"\"\"\n",
    "    print(f'Grouping faces from {len(image_files)} image(s)...')\n",
    "    \n",
    "    # Detect all faces\n",
    "    all_face_ids = []\n",
    "    face_info = []\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        print(f'\\nProcessing: {img_path}')\n",
    "        with open(img_path, 'rb') as image_stream:\n",
    "            faces = face_client.face.detect_with_stream(\n",
    "                image_stream,\n",
    "                detection_model='detection_03',\n",
    "                recognition_model='recognition_04'\n",
    "            )\n",
    "        \n",
    "        print(f'  Found {len(faces)} face(s)')\n",
    "        for face in faces:\n",
    "            all_face_ids.append(face.face_id)\n",
    "            face_info.append({\n",
    "                'face_id': face.face_id,\n",
    "                'image': img_path,\n",
    "                'rect': face.face_rectangle\n",
    "            })\n",
    "    \n",
    "    if len(all_face_ids) < 2:\n",
    "        print('\\n‚ö†Ô∏è  Need at least 2 faces for grouping')\n",
    "        return\n",
    "    \n",
    "    print(f'\\nTotal faces detected: {len(all_face_ids)}')\n",
    "    print('Grouping faces...')\n",
    "    \n",
    "    # Group faces\n",
    "    group_result = face_client.face.group(all_face_ids)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print('GROUPING RESULTS')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'Number of groups: {len(group_result.groups)}')\n",
    "    print(f'Messy group size: {len(group_result.messy_group)}')\n",
    "    \n",
    "    # Display each group\n",
    "    for i, group in enumerate(group_result.groups, 1):\n",
    "        print(f'\\nGroup {i}: {len(group)} face(s)')\n",
    "        for face_id in group:\n",
    "            info = next((f for f in face_info if f['face_id'] == face_id), None)\n",
    "            if info:\n",
    "                print(f'  - {info[\"image\"]}')\n",
    "    \n",
    "    if group_result.messy_group:\n",
    "        print(f'\\nMessy Group: {len(group_result.messy_group)} face(s)')\n",
    "        print('(Faces that could not be confidently grouped)')\n",
    "    \n",
    "    return group_result, face_info\n",
    "\n",
    "# Example: Group faces from multiple images\n",
    "images_to_group = [\n",
    "    'python/face-api/images/face1.jpg',\n",
    "    'python/face-api/images/face2.jpg',\n",
    "    'python/face-api/images/faces.jpg'\n",
    "]\n",
    "\n",
    "existing_images = [img for img in images_to_group if os.path.exists(img)]\n",
    "if len(existing_images) >= 1:\n",
    "    result, info = group_faces(existing_images)\n",
    "else:\n",
    "    print('Not enough images found for grouping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Face Landmarks\n",
    "\n",
    "### What are Face Landmarks?\n",
    "\n",
    "Face landmarks are specific points on a face that define its structure. The Face API returns 27 landmarks including:\n",
    "- Eye positions (pupils, inner/outer corners)\n",
    "- Eyebrow positions\n",
    "- Nose tip and root\n",
    "- Mouth corners and lips\n",
    "- Face outline points\n",
    "\n",
    "Landmarks are useful for:\n",
    "- Face alignment\n",
    "- Emotion analysis\n",
    "- Augmented reality filters\n",
    "- Facial animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_face_landmarks(image_file):\n",
    "    \"\"\"\n",
    "    Detect and visualize face landmarks.\n",
    "    \n",
    "    Args:\n",
    "        image_file: Path to image file\n",
    "    \"\"\"\n",
    "    print(f'Analyzing face landmarks in: {image_file}')\n",
    "    \n",
    "    # Detect face with landmarks\n",
    "    with open(image_file, 'rb') as image_stream:\n",
    "        faces = face_client.face.detect_with_stream(\n",
    "            image_stream,\n",
    "            return_face_landmarks=True,\n",
    "            detection_model='detection_03'\n",
    "        )\n",
    "    \n",
    "    if not faces:\n",
    "        print('‚ö†Ô∏è  No faces detected')\n",
    "        return\n",
    "    \n",
    "    print(f'‚úì Detected {len(faces)} face(s) with landmarks')\n",
    "    \n",
    "    # Process first face\n",
    "    face = faces[0]\n",
    "    landmarks = face.face_landmarks\n",
    "    \n",
    "    # Open image\n",
    "    img = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Define landmark groups and colors\n",
    "    landmark_groups = {\n",
    "        'Eyes': [\n",
    "            ('pupil_left', 'blue'),\n",
    "            ('pupil_right', 'blue'),\n",
    "            ('eye_left_outer', 'cyan'),\n",
    "            ('eye_left_top', 'cyan'),\n",
    "            ('eye_left_bottom', 'cyan'),\n",
    "            ('eye_left_inner', 'cyan'),\n",
    "            ('eye_right_inner', 'cyan'),\n",
    "            ('eye_right_top', 'cyan'),\n",
    "            ('eye_right_bottom', 'cyan'),\n",
    "            ('eye_right_outer', 'cyan')\n",
    "        ],\n",
    "        'Eyebrows': [\n",
    "            ('eyebrow_left_outer', 'green'),\n",
    "            ('eyebrow_left_inner', 'green'),\n",
    "            ('eyebrow_right_inner', 'green'),\n",
    "            ('eyebrow_right_outer', 'green')\n",
    "        ],\n",
    "        'Nose': [\n",
    "            ('nose_root_left', 'yellow'),\n",
    "            ('nose_root_right', 'yellow'),\n",
    "            ('nose_left_alar_top', 'yellow'),\n",
    "            ('nose_right_alar_top', 'yellow'),\n",
    "            ('nose_left_alar_out_tip', 'yellow'),\n",
    "            ('nose_right_alar_out_tip', 'yellow'),\n",
    "            ('nose_tip', 'orange')\n",
    "        ],\n",
    "        'Mouth': [\n",
    "            ('mouth_left', 'red'),\n",
    "            ('mouth_right', 'red'),\n",
    "            ('upper_lip_top', 'red'),\n",
    "            ('upper_lip_bottom', 'red'),\n",
    "            ('under_lip_top', 'red'),\n",
    "            ('under_lip_bottom', 'red')\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Draw landmarks\n",
    "    print(f'\\nLandmark Coordinates:')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    for group_name, landmark_list in landmark_groups.items():\n",
    "        print(f'\\n{group_name}:')\n",
    "        for landmark_name, color in landmark_list:\n",
    "            if hasattr(landmarks, landmark_name):\n",
    "                point = getattr(landmarks, landmark_name)\n",
    "                x, y = point.x, point.y\n",
    "                \n",
    "                # Draw point\n",
    "                radius = 3\n",
    "                draw.ellipse(\n",
    "                    [(x-radius, y-radius), (x+radius, y+radius)],\n",
    "                    fill=color,\n",
    "                    outline='white'\n",
    "                )\n",
    "                \n",
    "                print(f'  {landmark_name:25s}: ({x:6.1f}, {y:6.1f})')\n",
    "    \n",
    "    # Display annotated image\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Face Landmarks (27 Points)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_text = (\n",
    "        'üîµ Pupils  üî∑ Eyes\\n'\n",
    "        'üü¢ Eyebrows\\n'\n",
    "        'üü° Nose  üü† Nose Tip\\n'\n",
    "        'üî¥ Mouth'\n",
    "    )\n",
    "    plt.text(10, 30, legend_text, fontsize=10, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# Analyze landmarks\n",
    "image_file = 'python/face-api/images/face1.jpg'\n",
    "if os.path.exists(image_file):\n",
    "    faces_with_landmarks = analyze_face_landmarks(image_file)\n",
    "else:\n",
    "    print('Image not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark Applications\n",
    "\n",
    "Let's explore a practical application: calculating facial feature metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_facial_metrics(landmarks):\n",
    "    \"\"\"\n",
    "    Calculate useful metrics from face landmarks.\n",
    "    \"\"\"\n",
    "    # Inter-ocular distance (distance between pupils)\n",
    "    left_pupil = landmarks.pupil_left\n",
    "    right_pupil = landmarks.pupil_right\n",
    "    iod = np.sqrt((right_pupil.x - left_pupil.x)**2 + (right_pupil.y - left_pupil.y)**2)\n",
    "    \n",
    "    # Eye width (left eye)\n",
    "    left_eye_width = np.sqrt(\n",
    "        (landmarks.eye_left_outer.x - landmarks.eye_left_inner.x)**2 +\n",
    "        (landmarks.eye_left_outer.y - landmarks.eye_left_inner.y)**2\n",
    "    )\n",
    "    \n",
    "    # Mouth width\n",
    "    mouth_width = np.sqrt(\n",
    "        (landmarks.mouth_right.x - landmarks.mouth_left.x)**2 +\n",
    "        (landmarks.mouth_right.y - landmarks.mouth_left.y)**2\n",
    "    )\n",
    "    \n",
    "    # Nose length (approximate)\n",
    "    nose_length = np.sqrt(\n",
    "        (landmarks.nose_tip.x - landmarks.nose_root_left.x)**2 +\n",
    "        (landmarks.nose_tip.y - landmarks.nose_root_left.y)**2\n",
    "    )\n",
    "    \n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print('FACIAL METRICS')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'Inter-Ocular Distance: {iod:.2f} pixels')\n",
    "    print(f'Left Eye Width: {left_eye_width:.2f} pixels')\n",
    "    print(f'Mouth Width: {mouth_width:.2f} pixels')\n",
    "    print(f'Nose Length: {nose_length:.2f} pixels')\n",
    "    \n",
    "    # Calculate proportions\n",
    "    print(f'\\nFacial Proportions (relative to IOD):')\n",
    "    print(f'Eye Width Ratio: {left_eye_width/iod:.2f}')\n",
    "    print(f'Mouth Width Ratio: {mouth_width/iod:.2f}')\n",
    "    print(f'Nose Length Ratio: {nose_length/iod:.2f}')\n",
    "\n",
    "if 'faces_with_landmarks' in locals() and faces_with_landmarks:\n",
    "    calculate_facial_metrics(faces_with_landmarks[0].face_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Image Quality Assessment\n",
    "\n",
    "### Recognition Quality\n",
    "\n",
    "Not all face images are suitable for recognition. The Face API can assess image quality and return one of three levels:\n",
    "- **High**: Optimal for recognition\n",
    "- **Medium**: Acceptable for recognition\n",
    "- **Low**: Not recommended for recognition\n",
    "\n",
    "Quality is affected by:\n",
    "- Image resolution\n",
    "- Face size in image\n",
    "- Blur and noise\n",
    "- Face angle and occlusion\n",
    "- Lighting conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_face_quality(image_files):\n",
    "    \"\"\"\n",
    "    Assess the quality of faces for recognition purposes.\n",
    "    \n",
    "    Args:\n",
    "        image_files: List of image paths to assess\n",
    "    \"\"\"\n",
    "    print('Assessing face quality for recognition...')\n",
    "    print(f'{\"=\"*60}\\n')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        print(f'Image: {img_path}')\n",
    "        \n",
    "        with open(img_path, 'rb') as image_stream:\n",
    "            faces = face_client.face.detect_with_stream(\n",
    "                image_stream,\n",
    "                detection_model='detection_03',\n",
    "                recognition_model='recognition_04',\n",
    "                return_face_attributes=[FaceAttributeType.quality_for_recognition]\n",
    "            )\n",
    "        \n",
    "        if not faces:\n",
    "            print('  ‚ö†Ô∏è  No faces detected\\n')\n",
    "            continue\n",
    "        \n",
    "        for i, face in enumerate(faces, 1):\n",
    "            quality = face.face_attributes.quality_for_recognition\n",
    "            \n",
    "            # Quality indicator\n",
    "            if quality == QualityForRecognition.high:\n",
    "                indicator = '‚úÖ HIGH'\n",
    "                recommendation = 'Excellent for recognition'\n",
    "            elif quality == QualityForRecognition.medium:\n",
    "                indicator = '‚ö†Ô∏è  MEDIUM'\n",
    "                recommendation = 'Acceptable for recognition'\n",
    "            else:\n",
    "                indicator = '‚ùå LOW'\n",
    "                recommendation = 'Not recommended for recognition'\n",
    "            \n",
    "            print(f'  Face {i}: {indicator}')\n",
    "            print(f'           {recommendation}')\n",
    "            \n",
    "            results.append({\n",
    "                'image': img_path,\n",
    "                'quality': quality,\n",
    "                'face_rect': face.face_rectangle\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Assess quality of available images\n",
    "test_images = [\n",
    "    'python/face-api/images/face1.jpg',\n",
    "    'python/face-api/images/face2.jpg',\n",
    "    'python/face-api/images/faces.jpg'\n",
    "]\n",
    "\n",
    "quality_results = assess_face_quality(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Face Recognition Best Practices\n",
    "\n",
    "### Best Practices for Robust Face Recognition\n",
    "\n",
    "When building face recognition systems, follow these guidelines:\n",
    "\n",
    "#### 1. Image Quality\n",
    "- Use high-resolution images (minimum 200x200 pixels per face)\n",
    "- Ensure good lighting conditions\n",
    "- Avoid motion blur\n",
    "- Check quality_for_recognition before enrollment\n",
    "\n",
    "#### 2. Face Pose\n",
    "- Frontal faces work best (yaw, pitch, roll < ¬±45¬∞)\n",
    "- Avoid extreme angles\n",
    "- Ensure full face visibility\n",
    "\n",
    "#### 3. Detection Models\n",
    "- Use **detection_03** for best accuracy\n",
    "- Use **recognition_04** for most robust recognition\n",
    "\n",
    "#### 4. Multiple Images\n",
    "- Enroll multiple images per person when possible\n",
    "- Include variations in lighting, expression, and angle\n",
    "- Re-enroll periodically for aging faces\n",
    "\n",
    "#### 5. Verification Thresholds\n",
    "- High security: confidence > 0.7\n",
    "- Balanced: confidence > 0.6\n",
    "- Convenience: confidence > 0.5\n",
    "\n",
    "#### 6. Privacy and Ethics\n",
    "- Get explicit consent before enrolling faces\n",
    "- Implement data retention policies\n",
    "- Provide opt-out mechanisms\n",
    "- Comply with privacy regulations (GDPR, CCPA, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection and Recognition Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detection_models(image_file):\n",
    "    \"\"\"\n",
    "    Compare different detection models.\n",
    "    \"\"\"\n",
    "    print(f'Comparing detection models on: {image_file}')\n",
    "    print(f'{\"=\"*60}\\n')\n",
    "    \n",
    "    models = ['detection_01', 'detection_02', 'detection_03']\n",
    "    \n",
    "    for model in models:\n",
    "        print(f'Model: {model}')\n",
    "        \n",
    "        try:\n",
    "            with open(image_file, 'rb') as image_stream:\n",
    "                faces = face_client.face.detect_with_stream(\n",
    "                    image_stream,\n",
    "                    detection_model=model\n",
    "                )\n",
    "            \n",
    "            print(f'  Faces detected: {len(faces)}')\n",
    "            \n",
    "            if faces:\n",
    "                for i, face in enumerate(faces, 1):\n",
    "                    rect = face.face_rectangle\n",
    "                    print(f'    Face {i}: {rect.width}x{rect.height} at ({rect.left}, {rect.top})')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'  Error: {str(e)}')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print('\\nModel Characteristics:')\n",
    "    print('  detection_01: Optimized for speed')\n",
    "    print('  detection_02: Balanced speed and accuracy')\n",
    "    print('  detection_03: Highest accuracy, slightly slower')\n",
    "\n",
    "# Compare models\n",
    "test_image = 'python/face-api/images/face1.jpg'\n",
    "if os.path.exists(test_image):\n",
    "    compare_detection_models(test_image)\n",
    "else:\n",
    "    print('Test image not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comprehensive Face Analysis Pipeline\n",
    "\n",
    "Let's put it all together in a comprehensive analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_face_analysis(image_file):\n",
    "    \"\"\"\n",
    "    Perform comprehensive face analysis including detection, attributes, \n",
    "    landmarks, and quality assessment.\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"#\"*70}')\n",
    "    print(f'COMPREHENSIVE FACE ANALYSIS')\n",
    "    print(f'{\"#\"*70}')\n",
    "    print(f'Image: {image_file}\\n')\n",
    "    \n",
    "    # Step 1: Detect faces with all attributes\n",
    "    print('Step 1: Detecting faces...')\n",
    "    with open(image_file, 'rb') as image_stream:\n",
    "        faces = face_client.face.detect_with_stream(\n",
    "            image_stream,\n",
    "            detection_model='detection_03',\n",
    "            recognition_model='recognition_04',\n",
    "            return_face_landmarks=True,\n",
    "            return_face_attributes=[\n",
    "                FaceAttributeType.age,\n",
    "                FaceAttributeType.gender,\n",
    "                FaceAttributeType.emotion,\n",
    "                FaceAttributeType.quality_for_recognition,\n",
    "                FaceAttributeType.head_pose\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    if not faces:\n",
    "        print('‚ö†Ô∏è  No faces detected')\n",
    "        return\n",
    "    \n",
    "    print(f'‚úì Detected {len(faces)} face(s)\\n')\n",
    "    \n",
    "    # Step 2: Analyze each face\n",
    "    for i, face in enumerate(faces, 1):\n",
    "        print(f'{\"=\"*60}')\n",
    "        print(f'FACE {i} ANALYSIS')\n",
    "        print(f'{\"=\"*60}')\n",
    "        \n",
    "        attrs = face.face_attributes\n",
    "        \n",
    "        # Basic info\n",
    "        print(f'\\nüìç Location:')\n",
    "        rect = face.face_rectangle\n",
    "        print(f'   Position: ({rect.left}, {rect.top})')\n",
    "        print(f'   Size: {rect.width}x{rect.height} pixels')\n",
    "        \n",
    "        # Demographics\n",
    "        print(f'\\nüë§ Demographics:')\n",
    "        print(f'   Age: ~{attrs.age} years')\n",
    "        print(f'   Gender: {attrs.gender}')\n",
    "        \n",
    "        # Quality\n",
    "        print(f'\\n‚≠ê Recognition Quality: {attrs.quality_for_recognition}')\n",
    "        \n",
    "        # Emotion\n",
    "        emotions = {\n",
    "            'Happiness': attrs.emotion.happiness,\n",
    "            'Neutral': attrs.emotion.neutral,\n",
    "            'Sadness': attrs.emotion.sadness,\n",
    "            'Anger': attrs.emotion.anger\n",
    "        }\n",
    "        dominant = max(emotions, key=emotions.get)\n",
    "        print(f'\\nüòä Emotion: {dominant} ({emotions[dominant]:.1%})')\n",
    "        \n",
    "        # Head pose\n",
    "        print(f'\\nüîÑ Head Pose:')\n",
    "        print(f'   Pitch: {attrs.head_pose.pitch:+.1f}¬∞ (up/down)')\n",
    "        print(f'   Yaw: {attrs.head_pose.yaw:+.1f}¬∞ (left/right)')\n",
    "        print(f'   Roll: {attrs.head_pose.roll:+.1f}¬∞ (tilt)')\n",
    "        \n",
    "        # Landmarks summary\n",
    "        if face.face_landmarks:\n",
    "            print(f'\\nüìç Landmarks: 27 points detected')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Step 3: Visualize\n",
    "    print('Step 2: Creating visualization...')\n",
    "    \n",
    "    img = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    for face in faces:\n",
    "        rect = face.face_rectangle\n",
    "        left = rect.left\n",
    "        top = rect.top\n",
    "        right = left + rect.width\n",
    "        bottom = top + rect.height\n",
    "        \n",
    "        # Draw bounding box\n",
    "        draw.rectangle([(left, top), (right, bottom)], outline='green', width=3)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        if face.face_landmarks:\n",
    "            landmarks = face.face_landmarks\n",
    "            # Draw key points\n",
    "            key_points = [\n",
    "                landmarks.pupil_left,\n",
    "                landmarks.pupil_right,\n",
    "                landmarks.nose_tip,\n",
    "                landmarks.mouth_left,\n",
    "                landmarks.mouth_right\n",
    "            ]\n",
    "            \n",
    "            for point in key_points:\n",
    "                x, y = point.x, point.y\n",
    "                draw.ellipse([(x-2, y-2), (x+2, y+2)], fill='red')\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Comprehensive Face Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    print('‚úì Analysis complete!')\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# Run comprehensive analysis\n",
    "test_image = 'python/face-api/images/face1.jpg'\n",
    "if os.path.exists(test_image):\n",
    "    analysis_results = comprehensive_face_analysis(test_image)\n",
    "else:\n",
    "    print('Test image not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this advanced lab, you learned:\n",
    "\n",
    "‚úÖ **Face Verification**: Determine if two faces belong to the same person (1:1 matching)  \n",
    "‚úÖ **Similar Face Finding**: Search for similar faces in a collection (1:N matching)  \n",
    "‚úÖ **Face Grouping**: Automatically organize faces into groups based on similarity  \n",
    "‚úÖ **Face Landmarks**: Analyze 27 facial feature points for detailed face structure  \n",
    "‚úÖ **Quality Assessment**: Evaluate face image quality for recognition purposes  \n",
    "‚úÖ **Best Practices**: Guidelines for building robust face recognition systems  \n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Face IDs**: Temporary identifiers valid for 24 hours, used for comparison operations\n",
    "- **Confidence Scores**: Range from 0-1, indicating match certainty\n",
    "- **Detection Models**: Choose based on speed vs. accuracy requirements\n",
    "- **Recognition Models**: Use the latest model (recognition_04) for best results\n",
    "- **Quality Matters**: Image quality significantly impacts recognition accuracy\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "These advanced capabilities enable:\n",
    "- **Security Systems**: Access control and identity verification\n",
    "- **Photo Management**: Automatic face tagging and organization\n",
    "- **Social Media**: Face recognition in photos and videos\n",
    "- **Retail Analytics**: Customer recognition and demographics\n",
    "- **Law Enforcement**: Person of interest identification (with proper authorization)\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "‚ö†Ô∏è **Privacy and Ethics**\n",
    "- Always obtain consent before enrolling faces\n",
    "- Implement proper data protection measures\n",
    "- Comply with relevant privacy regulations\n",
    "- Consider bias and fairness in face recognition\n",
    "- Provide transparency about how face data is used\n",
    "\n",
    "‚ö†Ô∏è **Technical Limitations**\n",
    "- Accuracy decreases with poor image quality\n",
    "- Extreme poses and occlusions affect performance\n",
    "- Recognition accuracy varies across demographics\n",
    "- Face IDs expire after 24 hours\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Azure Face API Documentation](https://docs.microsoft.com/azure/cognitive-services/face/)\n",
    "- [Face API Quickstarts](https://docs.microsoft.com/azure/cognitive-services/face/quickstarts/client-libraries)\n",
    "- [Responsible AI Guidelines](https://www.microsoft.com/ai/responsible-ai)\n",
    "- [Face API Limits and Quotas](https://docs.microsoft.com/azure/cognitive-services/face/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. **Build a Face Matcher**: Create a function that takes a target face and finds all matching faces from a collection with confidence > 0.6\n",
    "\n",
    "2. **Quality Filter**: Write code that only accepts faces with \"high\" quality_for_recognition for enrollment\n",
    "\n",
    "3. **Emotion Tracker**: Analyze multiple images and track emotion changes over time\n",
    "\n",
    "4. **Face Alignment**: Use landmarks to calculate the angle needed to align a face to a standard orientation\n",
    "\n",
    "5. **Demographics Dashboard**: Create a summary dashboard showing age distribution and gender breakdown from multiple faces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
