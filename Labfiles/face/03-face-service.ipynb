{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Face Service - Face Detection and Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Azure AI Face service provides AI algorithms that detect, recognize, and analyze human faces in images. Face detection is a key capability that enables many scenarios, from access control to demographics analysis.\n",
    "\n",
    "In this lab, you will:\n",
    "- Detect faces in images using the Azure Face API\n",
    "- Analyze face attributes such as age, emotion, accessories, and more\n",
    "- Visualize detected faces with bounding boxes\n",
    "- Understand the data returned by the Face API\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure subscription with Azure AI Services resource\n",
    "- Python 3.7 or later\n",
    "- Required packages: azure-cognitiveservices-vision-face, python-dotenv, matplotlib, pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages\n",
    "\n",
    "First, let's install the necessary packages for working with the Azure Face API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install azure-cognitiveservices-vision-face python-dotenv matplotlib pillow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "Import all the libraries we'll need for face detection and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from azure.cognitiveservices.vision.face.models import FaceAttributeType\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import numpy as np\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration Settings\n",
    "\n",
    "Load the Azure AI Services endpoint and key from the `.env` file. Make sure you've created a `.env` file in the `python/face-api/` directory with your credentials:\n",
    "\n",
    "```\n",
    "AI_SERVICE_ENDPOINT=your_ai_services_endpoint\n",
    "AI_SERVICE_KEY=your_ai_services_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv('python/face-api/.env')\n",
    "\n",
    "# Get Azure AI Services endpoint and key\n",
    "cog_endpoint = os.getenv('AI_SERVICE_ENDPOINT')\n",
    "cog_key = os.getenv('AI_SERVICE_KEY')\n",
    "\n",
    "# Verify credentials are loaded\n",
    "if cog_endpoint and cog_key:\n",
    "    print(f'‚úì Endpoint loaded: {cog_endpoint}')\n",
    "    print('‚úì API key loaded successfully')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Warning: Credentials not found. Please check your .env file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Authenticate Face Client\n",
    "\n",
    "Create a Face client using the endpoint and key. This client will be used to make API calls to the Azure Face service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials object\n",
    "credentials = CognitiveServicesCredentials(cog_key)\n",
    "\n",
    "# Create Face client\n",
    "face_client = FaceClient(cog_endpoint, credentials)\n",
    "\n",
    "print('‚úì Face client authenticated successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Basic Face Detection\n",
    "\n",
    "Let's start with the simplest use case - detecting faces in an image. We'll use the `detect_with_stream` method which returns basic face information including the bounding box location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the image file\n",
    "image_file = 'python/face-api/images/face1.jpg'\n",
    "\n",
    "# Display the original image\n",
    "print('Analyzing image:', image_file)\n",
    "img = Image.open(image_file)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "plt.show()\n",
    "\n",
    "# Detect faces (basic detection without attributes)\n",
    "with open(image_file, 'rb') as image_stream:\n",
    "    detected_faces = face_client.face.detect_with_stream(image_stream)\n",
    "\n",
    "print(f'\\n‚úì Found {len(detected_faces)} face(s) in the image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Face Detection Results\n",
    "\n",
    "Each detected face includes:\n",
    "- **face_id**: A unique identifier for the detected face (valid for 24 hours)\n",
    "- **face_rectangle**: The bounding box coordinates (left, top, width, height)\n",
    "\n",
    "Let's examine the detected face data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display details of detected faces\n",
    "for i, face in enumerate(detected_faces, 1):\n",
    "    print(f'\\nFace {i}:')\n",
    "    print(f'  Face ID: {face.face_id}')\n",
    "    rect = face.face_rectangle\n",
    "    print(f'  Location: Left={rect.left}, Top={rect.top}')\n",
    "    print(f'  Size: Width={rect.width}, Height={rect.height}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Detected Faces\n",
    "\n",
    "Now let's visualize the detected faces by drawing bounding boxes around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_faces(image_file, detected_faces, title='Detected Faces'):\n",
    "    \"\"\"\n",
    "    Visualize detected faces with bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        image_file: Path to the image file\n",
    "        detected_faces: List of detected face objects from Face API\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Open image and create drawing context\n",
    "    img = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Define colors for bounding boxes\n",
    "    color = 'lightgreen'\n",
    "    \n",
    "    # Draw a bounding box around each face\n",
    "    for i, face in enumerate(detected_faces, 1):\n",
    "        rect = face.face_rectangle\n",
    "        left = rect.left\n",
    "        top = rect.top\n",
    "        right = left + rect.width\n",
    "        bottom = top + rect.height\n",
    "        \n",
    "        # Draw rectangle\n",
    "        draw.rectangle([(left, top), (right, bottom)], outline=color, width=5)\n",
    "        \n",
    "        # Add face number label\n",
    "        draw.text((left, top - 20), f'Face {i}', fill=color)\n",
    "    \n",
    "    # Display the annotated image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Visualize the detected faces\n",
    "annotated_img = visualize_faces(image_file, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Face Detection with Attributes\n",
    "\n",
    "The Face API can analyze various face attributes beyond just detection. Let's detect faces and request specific attributes:\n",
    "\n",
    "- **Age**: Estimated age of the person\n",
    "- **Gender**: Detected gender\n",
    "- **Emotion**: Emotional state (happiness, sadness, anger, etc.)\n",
    "- **Glasses**: Type of glasses (if any)\n",
    "- **Hair**: Hair color and baldness\n",
    "- **Facial Hair**: Presence of mustache, beard, or sideburns\n",
    "- **Accessories**: Accessories like glasses, masks, etc.\n",
    "- **Makeup**: Presence of eye or lip makeup\n",
    "- **Smile**: Smile intensity\n",
    "- **Head Pose**: Head orientation (pitch, roll, yaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the face attributes we want to analyze\n",
    "face_attributes = [\n",
    "    FaceAttributeType.age,\n",
    "    FaceAttributeType.gender,\n",
    "    FaceAttributeType.emotion,\n",
    "    FaceAttributeType.glasses,\n",
    "    FaceAttributeType.hair,\n",
    "    FaceAttributeType.facial_hair,\n",
    "    FaceAttributeType.accessories,\n",
    "    FaceAttributeType.makeup,\n",
    "    FaceAttributeType.smile,\n",
    "    FaceAttributeType.head_pose\n",
    "]\n",
    "\n",
    "# Detect faces with attributes\n",
    "print(f'Analyzing faces with attributes in: {image_file}')\n",
    "with open(image_file, 'rb') as image_stream:\n",
    "    detected_faces_with_attrs = face_client.face.detect_with_stream(\n",
    "        image_stream,\n",
    "        return_face_attributes=face_attributes\n",
    "    )\n",
    "\n",
    "print(f'\\n‚úì Detected {len(detected_faces_with_attrs)} face(s) with attributes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Face Attributes\n",
    "\n",
    "Let's examine the detailed attributes for each detected face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_face_attributes(face, face_number):\n",
    "    \"\"\"\n",
    "    Display detailed analysis of face attributes.\n",
    "    \n",
    "    Args:\n",
    "        face: Detected face object with attributes\n",
    "        face_number: Face number for display\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FACE {face_number} ANALYSIS')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    attrs = face.face_attributes\n",
    "    \n",
    "    # Basic demographics\n",
    "    print(f'\\nüìä Demographics:')\n",
    "    print(f'   Age: {attrs.age} years old')\n",
    "    print(f'   Gender: {attrs.gender}')\n",
    "    \n",
    "    # Emotions\n",
    "    print(f'\\nüòä Emotions:')\n",
    "    emotions = {\n",
    "        'Happiness': attrs.emotion.happiness,\n",
    "        'Sadness': attrs.emotion.sadness,\n",
    "        'Anger': attrs.emotion.anger,\n",
    "        'Contempt': attrs.emotion.contempt,\n",
    "        'Disgust': attrs.emotion.disgust,\n",
    "        'Fear': attrs.emotion.fear,\n",
    "        'Surprise': attrs.emotion.surprise,\n",
    "        'Neutral': attrs.emotion.neutral\n",
    "    }\n",
    "    \n",
    "    # Find dominant emotion\n",
    "    dominant_emotion = max(emotions, key=emotions.get)\n",
    "    print(f'   Dominant Emotion: {dominant_emotion} ({emotions[dominant_emotion]:.2%})')\n",
    "    \n",
    "    # Display all emotions\n",
    "    print(f'   All Emotions:')\n",
    "    for emotion, value in sorted(emotions.items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = '‚ñà' * int(value * 20)\n",
    "        print(f'      {emotion:12s}: {bar} {value:.2%}')\n",
    "    \n",
    "    # Facial features\n",
    "    print(f'\\nüëì Accessories & Features:')\n",
    "    print(f'   Glasses: {attrs.glasses}')\n",
    "    print(f'   Smile: {attrs.smile:.2%}')\n",
    "    \n",
    "    # Hair\n",
    "    print(f'\\nüíá Hair:')\n",
    "    if attrs.hair.hair_color:\n",
    "        print(f'   Hair Colors:')\n",
    "        for color in sorted(attrs.hair.hair_color, key=lambda x: x.confidence, reverse=True):\n",
    "            print(f'      {color.color}: {color.confidence:.2%}')\n",
    "    print(f'   Bald: {attrs.hair.bald:.2%}')\n",
    "    \n",
    "    # Facial hair\n",
    "    print(f'\\nüßî Facial Hair:')\n",
    "    print(f'   Mustache: {attrs.facial_hair.moustache:.2%}')\n",
    "    print(f'   Beard: {attrs.facial_hair.beard:.2%}')\n",
    "    print(f'   Sideburns: {attrs.facial_hair.sideburns:.2%}')\n",
    "    \n",
    "    # Makeup\n",
    "    print(f'\\nüíÑ Makeup:')\n",
    "    print(f'   Eye Makeup: {\"Yes\" if attrs.makeup.eye_makeup else \"No\"}')\n",
    "    print(f'   Lip Makeup: {\"Yes\" if attrs.makeup.lip_makeup else \"No\"}')\n",
    "    \n",
    "    # Accessories\n",
    "    print(f'\\nüëë Accessories:')\n",
    "    if attrs.accessories:\n",
    "        for accessory in attrs.accessories:\n",
    "            print(f'   {accessory.type}: {accessory.confidence:.2%}')\n",
    "    else:\n",
    "        print(f'   None detected')\n",
    "    \n",
    "    # Head pose\n",
    "    print(f'\\nüîÑ Head Pose:')\n",
    "    print(f'   Pitch: {attrs.head_pose.pitch:.1f}¬∞')\n",
    "    print(f'   Roll: {attrs.head_pose.roll:.1f}¬∞')\n",
    "    print(f'   Yaw: {attrs.head_pose.yaw:.1f}¬∞')\n",
    "\n",
    "# Analyze each detected face\n",
    "for i, face in enumerate(detected_faces_with_attrs, 1):\n",
    "    analyze_face_attributes(face, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Faces with Attribute Labels\n",
    "\n",
    "Let's create an enhanced visualization that shows key attributes alongside the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_faces_with_attributes(image_file, detected_faces):\n",
    "    \"\"\"\n",
    "    Visualize detected faces with attribute labels.\n",
    "    \"\"\"\n",
    "    # Open image\n",
    "    img = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Process each face\n",
    "    for i, face in enumerate(detected_faces, 1):\n",
    "        rect = face.face_rectangle\n",
    "        left = rect.left\n",
    "        top = rect.top\n",
    "        right = left + rect.width\n",
    "        bottom = top + rect.height\n",
    "        \n",
    "        # Draw bounding box\n",
    "        draw.rectangle([(left, top), (right, bottom)], outline='lightgreen', width=5)\n",
    "        \n",
    "        # Get attributes\n",
    "        attrs = face.face_attributes\n",
    "        \n",
    "        # Find dominant emotion\n",
    "        emotions = {\n",
    "            'Happy': attrs.emotion.happiness,\n",
    "            'Sad': attrs.emotion.sadness,\n",
    "            'Angry': attrs.emotion.anger,\n",
    "            'Neutral': attrs.emotion.neutral\n",
    "        }\n",
    "        dominant_emotion = max(emotions, key=emotions.get)\n",
    "        \n",
    "        # Create label text\n",
    "        label_lines = [\n",
    "            f'Face {i}',\n",
    "            f'Age: {int(attrs.age)}',\n",
    "            f'{attrs.gender}',\n",
    "            f'{dominant_emotion}',\n",
    "            f'{attrs.glasses}'\n",
    "        ]\n",
    "        \n",
    "        # Draw labels\n",
    "        y_offset = top - 90\n",
    "        for line in label_lines:\n",
    "            draw.text((left, y_offset), line, fill='lightgreen')\n",
    "            y_offset += 15\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.title('Face Detection with Attributes')\n",
    "    plt.show()\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Visualize faces with attributes\n",
    "annotated_img = visualize_faces_with_attributes(image_file, detected_faces_with_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Multiple Images\n",
    "\n",
    "Let's try analyzing different images to see how the Face API handles various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available images\n",
    "image_files = [\n",
    "    'python/face-api/images/face1.jpg',\n",
    "    'python/face-api/images/face2.jpg',\n",
    "    'python/face-api/images/faces.jpg'\n",
    "]\n",
    "\n",
    "# Process each image\n",
    "for image_file in image_files:\n",
    "    if os.path.exists(image_file):\n",
    "        print(f'\\n{\"#\"*70}')\n",
    "        print(f'Processing: {image_file}')\n",
    "        print(f'{\"#\"*70}')\n",
    "        \n",
    "        # Detect faces\n",
    "        with open(image_file, 'rb') as image_stream:\n",
    "            faces = face_client.face.detect_with_stream(\n",
    "                image_stream,\n",
    "                return_face_attributes=[FaceAttributeType.age, \n",
    "                                       FaceAttributeType.gender,\n",
    "                                       FaceAttributeType.emotion]\n",
    "            )\n",
    "        \n",
    "        print(f'Found {len(faces)} face(s)')\n",
    "        \n",
    "        # Visualize\n",
    "        if faces:\n",
    "            visualize_faces_with_attributes(image_file, faces)\n",
    "    else:\n",
    "        print(f'Image not found: {image_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Emotion Analysis Deep Dive\n",
    "\n",
    "Let's create a detailed emotion analysis visualization to better understand the emotional state detected in faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_chart(face, face_number):\n",
    "    \"\"\"\n",
    "    Create a bar chart showing emotion scores for a face.\n",
    "    \"\"\"\n",
    "    emotions = face.face_attributes.emotion\n",
    "    \n",
    "    emotion_dict = {\n",
    "        'Happiness': emotions.happiness,\n",
    "        'Neutral': emotions.neutral,\n",
    "        'Sadness': emotions.sadness,\n",
    "        'Anger': emotions.anger,\n",
    "        'Surprise': emotions.surprise,\n",
    "        'Fear': emotions.fear,\n",
    "        'Contempt': emotions.contempt,\n",
    "        'Disgust': emotions.disgust\n",
    "    }\n",
    "    \n",
    "    # Sort by value\n",
    "    sorted_emotions = dict(sorted(emotion_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['#2ecc71' if v == max(sorted_emotions.values()) else '#3498db' \n",
    "              for v in sorted_emotions.values()]\n",
    "    plt.barh(list(sorted_emotions.keys()), list(sorted_emotions.values()), color=colors)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.title(f'Emotion Analysis - Face {face_number}')\n",
    "    plt.xlim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (emotion, value) in enumerate(sorted_emotions.items()):\n",
    "        plt.text(value + 0.02, i, f'{value:.2%}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot emotion chart for first face\n",
    "if detected_faces_with_attrs:\n",
    "    plot_emotion_chart(detected_faces_with_attrs[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "‚úÖ Set up and authenticate the Azure Face API client  \n",
    "‚úÖ Detect faces in images and get bounding box coordinates  \n",
    "‚úÖ Analyze comprehensive face attributes including age, gender, emotion, and more  \n",
    "‚úÖ Visualize detected faces with bounding boxes and labels  \n",
    "‚úÖ Create detailed emotion analysis charts  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- The Face API can detect multiple faces in a single image\n",
    "- Face attributes include demographics, emotions, accessories, and physical features\n",
    "- Emotion detection returns confidence scores for 8 different emotions\n",
    "- Face IDs are temporary (24 hours) and used for comparison operations\n",
    "- The API provides rich data that can be used for various AI applications\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the **Advanced Topics Notebook** to learn about:\n",
    "- Face comparison and verification\n",
    "- Finding similar faces\n",
    "- Face grouping\n",
    "- Detailed facial landmarks\n",
    "- Face recognition best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "If needed, you can save the annotated images to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last annotated image\n",
    "if 'annotated_img' in locals():\n",
    "    output_file = 'detected_faces_output.jpg'\n",
    "    annotated_img.save(output_file)\n",
    "    print(f'‚úì Annotated image saved to: {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
