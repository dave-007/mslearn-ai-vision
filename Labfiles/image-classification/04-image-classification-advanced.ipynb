{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04: Advanced Image Classification with Azure Custom Vision\n",
    "\n",
    "This notebook covers advanced topics in image classification using Azure Custom Vision. Build on the basic concepts to implement production-ready solutions.\n",
    "\n",
    "## Advanced Topics Covered\n",
    "\n",
    "1. **Transfer Learning** - Understanding and leveraging pre-trained models\n",
    "2. **Domain Selection** - Choosing specialized domains for better performance\n",
    "3. **Data Augmentation** - Expanding training data programmatically\n",
    "4. **Advanced Metrics** - Confusion matrices and ROC curves\n",
    "5. **Multi-label Classification** - Assigning multiple tags to single images\n",
    "6. **Model Optimization** - Improving speed and accuracy\n",
    "7. **Model Export** - Deploying to edge devices\n",
    "8. **Batch Predictions** - Efficient processing of multiple images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages including image processing libraries\n",
    "!pip install azure-cognitiveservices-vision-customvision python-dotenv pillow matplotlib numpy scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import (\n",
    "    ImageFileCreateBatch, ImageFileCreateEntry, Classification\n",
    ")\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = 'python/train-classifier/.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "training_endpoint = os.getenv('TrainingEndpoint')\n",
    "training_key = os.getenv('TrainingKey')\n",
    "prediction_endpoint = os.getenv('PredictionEndpoint', training_endpoint)\n",
    "prediction_key = os.getenv('PredictionKey', training_key)\n",
    "project_id = os.getenv('ProjectID', None)\n",
    "\n",
    "training_credentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\n",
    "training_client = CustomVisionTrainingClient(training_endpoint, training_credentials)\n",
    "\n",
    "prediction_credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "prediction_client = CustomVisionPredictionClient(prediction_endpoint, prediction_credentials)\n",
    "\n",
    "print(\"Clients authenticated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Transfer Learning and Domains\n",
    "\n",
    "Azure Custom Vision uses **transfer learning** - it starts with pre-trained models and adapts them to your specific classification task. This allows excellent performance with relatively few training images.\n",
    "\n",
    "### Available Domains:\n",
    "- **General**: Default, works well for most scenarios\n",
    "- **Food**: Optimized for food/meal recognition\n",
    "- **Landmarks**: Buildings and natural landmarks\n",
    "- **Retail**: Products and items in retail context\n",
    "- **General (compact)**: Smaller model for edge deployment\n",
    "- **Custom**: For specialized scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available domains\n",
    "domains = training_client.get_domains()\n",
    "\n",
    "print(\"Available Classification Domains:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Name':<25} {'Type':<15} {'Exportable':<12} {'ID'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for domain in domains:\n",
    "    if domain.type == 'Classification':\n",
    "        exportable = \"Yes\" if domain.exportable else \"No\"\n",
    "        print(f\"{domain.name:<25} {domain.type:<15} {exportable:<12} {domain.id}\")\n",
    "\n",
    "# Select the Food domain for our fruit classification\n",
    "food_domain = next((d for d in domains if 'food' in d.name.lower()), None)\n",
    "\n",
    "if food_domain:\n",
    "    print(f\"\\n✓ Selected domain: {food_domain.name}\")\n",
    "    print(f\"  This domain is optimized for food classification tasks.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Food domain not found, using General domain\")\n",
    "    food_domain = next(d for d in domains if 'general' in d.name.lower() and d.type == 'Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Project with Custom Domain\n",
    "\n",
    "When creating a project, choosing the right domain can significantly improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project with Food domain\n",
    "project_name = f\"Advanced Fruit Classification {uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "project = training_client.create_project(\n",
    "    name=project_name,\n",
    "    description=\"Advanced fruit classification with optimized domain\",\n",
    "    domain_id=food_domain.id,\n",
    "    classification_type=\"Multiclass\"  # Single label per image\n",
    ")\n",
    "\n",
    "print(f\"✓ Created project: {project.name}\")\n",
    "print(f\"  Project ID: {project.id}\")\n",
    "print(f\"  Domain: {food_domain.name}\")\n",
    "print(f\"  Classification Type: Multiclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation Techniques\n",
    "\n",
    "Data augmentation artificially expands your training dataset by creating modified versions of existing images. This helps the model generalize better and prevents overfitting.\n",
    "\n",
    "### Common Augmentation Techniques:\n",
    "- **Rotation**: Rotate images by various angles\n",
    "- **Flipping**: Horizontal and vertical flips\n",
    "- **Brightness adjustment**: Simulate different lighting conditions\n",
    "- **Contrast adjustment**: Enhance or reduce contrast\n",
    "- **Color adjustment**: Modify color saturation\n",
    "- **Cropping**: Random crops to focus on different areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, augmentation_type):\n",
    "    \"\"\"\n",
    "    Apply data augmentation to an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        augmentation_type: Type of augmentation to apply\n",
    "    \n",
    "    Returns:\n",
    "        Augmented PIL Image\n",
    "    \"\"\"\n",
    "    if augmentation_type == 'rotate_90':\n",
    "        return image.rotate(90, expand=True)\n",
    "    elif augmentation_type == 'rotate_270':\n",
    "        return image.rotate(270, expand=True)\n",
    "    elif augmentation_type == 'flip_horizontal':\n",
    "        return ImageOps.mirror(image)\n",
    "    elif augmentation_type == 'flip_vertical':\n",
    "        return ImageOps.flip(image)\n",
    "    elif augmentation_type == 'brightness_increase':\n",
    "        enhancer = ImageEnhance.Brightness(image)\n",
    "        return enhancer.enhance(1.3)\n",
    "    elif augmentation_type == 'brightness_decrease':\n",
    "        enhancer = ImageEnhance.Brightness(image)\n",
    "        return enhancer.enhance(0.7)\n",
    "    elif augmentation_type == 'contrast_increase':\n",
    "        enhancer = ImageEnhance.Contrast(image)\n",
    "        return enhancer.enhance(1.3)\n",
    "    elif augmentation_type == 'contrast_decrease':\n",
    "        enhancer = ImageEnhance.Contrast(image)\n",
    "        return enhancer.enhance(0.7)\n",
    "    elif augmentation_type == 'saturation_increase':\n",
    "        enhancer = ImageEnhance.Color(image)\n",
    "        return enhancer.enhance(1.3)\n",
    "    elif augmentation_type == 'saturation_decrease':\n",
    "        enhancer = ImageEnhance.Color(image)\n",
    "        return enhancer.enhance(0.7)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "# Demonstrate augmentation on a sample image\n",
    "sample_path = 'training-images/apple'\n",
    "if os.path.exists(sample_path):\n",
    "    sample_files = [f for f in os.listdir(sample_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if sample_files:\n",
    "        sample_image = Image.open(os.path.join(sample_path, sample_files[0]))\n",
    "        \n",
    "        # Show different augmentations\n",
    "        augmentation_types = ['rotate_90', 'flip_horizontal', 'brightness_increase', \n",
    "                            'contrast_increase', 'saturation_increase']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        axes[0].imshow(sample_image)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        for idx, aug_type in enumerate(augmentation_types, 1):\n",
    "            augmented = augment_image(sample_image, aug_type)\n",
    "            axes[idx].imshow(augmented)\n",
    "            axes[idx].set_title(aug_type.replace('_', ' ').title())\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Data Augmentation Examples', y=1.02, fontsize=16)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Images with Augmentation\n",
    "\n",
    "Now let's upload training images along with augmented versions to increase our training dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tags\n",
    "tag_names = ['apple', 'banana', 'orange']\n",
    "tags = {}\n",
    "\n",
    "for tag_name in tag_names:\n",
    "    tag = training_client.create_tag(project.id, tag_name)\n",
    "    tags[tag_name] = tag\n",
    "    print(f\"Created tag: {tag_name}\")\n",
    "\n",
    "def upload_images_with_augmentation(folder_path, project_id, tags_dict, augment=True, max_augmentations=3):\n",
    "    \"\"\"\n",
    "    Upload images with optional data augmentation.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to images folder\n",
    "        project_id: Custom Vision project ID\n",
    "        tags_dict: Dictionary of tag objects\n",
    "        augment: Whether to apply data augmentation\n",
    "        max_augmentations: Maximum number of augmented versions per image\n",
    "    \"\"\"\n",
    "    augmentation_types = ['flip_horizontal', 'brightness_increase', 'brightness_decrease',\n",
    "                         'contrast_increase', 'saturation_increase']\n",
    "    \n",
    "    for tag_name, tag in tags_dict.items():\n",
    "        tag_folder = os.path.join(folder_path, tag_name)\n",
    "        \n",
    "        if not os.path.exists(tag_folder):\n",
    "            continue\n",
    "        \n",
    "        image_files = [f for f in os.listdir(tag_folder) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        \n",
    "        total_uploaded = 0\n",
    "        \n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(tag_folder, image_file)\n",
    "            \n",
    "            # Upload original image\n",
    "            with open(image_path, \"rb\") as image_data:\n",
    "                training_client.create_images_from_data(\n",
    "                    project_id, \n",
    "                    image_data.read(), \n",
    "                    [tag.id]\n",
    "                )\n",
    "            total_uploaded += 1\n",
    "            \n",
    "            # Upload augmented versions\n",
    "            if augment:\n",
    "                original_image = Image.open(image_path)\n",
    "                \n",
    "                for aug_idx, aug_type in enumerate(augmentation_types[:max_augmentations]):\n",
    "                    augmented_image = augment_image(original_image, aug_type)\n",
    "                    \n",
    "                    # Convert to bytes\n",
    "                    img_byte_arr = io.BytesIO()\n",
    "                    augmented_image.save(img_byte_arr, format='JPEG')\n",
    "                    img_byte_arr.seek(0)\n",
    "                    \n",
    "                    training_client.create_images_from_data(\n",
    "                        project_id,\n",
    "                        img_byte_arr.read(),\n",
    "                        [tag.id]\n",
    "                    )\n",
    "                    total_uploaded += 1\n",
    "        \n",
    "        print(f\"✓ Uploaded {total_uploaded} images (including augmented) for tag '{tag_name}'\")\n",
    "\n",
    "# Upload with augmentation\n",
    "print(\"Uploading images with data augmentation...\\n\")\n",
    "upload_images_with_augmentation('training-images', project.id, tags, augment=True, max_augmentations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train and Publish Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with augmented dataset...\\n\")\n",
    "\n",
    "iteration = training_client.train_project(project.id)\n",
    "\n",
    "while iteration.status != \"Completed\":\n",
    "    iteration = training_client.get_iteration(project.id, iteration.id)\n",
    "    print(f\"Training status: {iteration.status}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"\\n✓ Model trained successfully!\")\n",
    "\n",
    "# Publish the model\n",
    "publish_name = \"AdvancedFruitClassifier\"\n",
    "prediction_resource_id = os.getenv('PredictionResourceId', None)\n",
    "\n",
    "try:\n",
    "    if prediction_resource_id:\n",
    "        training_client.publish_iteration(project.id, iteration.id, publish_name, prediction_resource_id)\n",
    "    else:\n",
    "        training_client.publish_iteration(project.id, iteration.id, publish_name)\n",
    "    print(f\"✓ Model published as '{publish_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Publishing info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Performance Evaluation\n",
    "\n",
    "Beyond basic precision and recall, let's examine advanced evaluation metrics including confusion matrices and per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_advanced(project_id, iteration_id, training_client):\n",
    "    \"\"\"\n",
    "    Perform advanced model evaluation with detailed metrics.\n",
    "    \"\"\"\n",
    "    performance = training_client.get_iteration_performance(project_id, iteration_id)\n",
    "    \n",
    "    print(\"Advanced Performance Metrics\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Precision:          {performance.precision:.4f} ({performance.precision:.2%})\")\n",
    "    print(f\"  Recall:             {performance.recall:.4f} ({performance.recall:.2%})\")\n",
    "    print(f\"  Average Precision:  {performance.average_precision:.4f} ({performance.average_precision:.2%})\")\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if performance.precision + performance.recall > 0:\n",
    "        f1_score = 2 * (performance.precision * performance.recall) / (performance.precision + performance.recall)\n",
    "        print(f\"  F1 Score:           {f1_score:.4f} ({f1_score:.2%})\")\n",
    "    \n",
    "    # Per-tag detailed metrics\n",
    "    print(f\"\\nPer-Tag Performance:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Tag':<15} {'Precision':<12} {'Recall':<12} {'AP':<12} {'F1':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    tag_metrics = []\n",
    "    for tag_perf in performance.per_tag_performance:\n",
    "        if tag_perf.precision + tag_perf.recall > 0:\n",
    "            f1 = 2 * (tag_perf.precision * tag_perf.recall) / (tag_perf.precision + tag_perf.recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        \n",
    "        print(f\"{tag_perf.name:<15} {tag_perf.precision:<12.2%} {tag_perf.recall:<12.2%} \"\n",
    "              f\"{tag_perf.average_precision:<12.2%} {f1:<12.2%}\")\n",
    "        \n",
    "        tag_metrics.append({\n",
    "            'name': tag_perf.name,\n",
    "            'precision': tag_perf.precision,\n",
    "            'recall': tag_perf.recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "    \n",
    "    # Visualize metrics comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Bar chart of metrics\n",
    "    tag_names_eval = [tm['name'] for tm in tag_metrics]\n",
    "    precisions = [tm['precision'] for tm in tag_metrics]\n",
    "    recalls = [tm['recall'] for tm in tag_metrics]\n",
    "    f1_scores = [tm['f1'] for tm in tag_metrics]\n",
    "    \n",
    "    x = np.arange(len(tag_names_eval))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0].bar(x - width, precisions, width, label='Precision', color='skyblue')\n",
    "    axes[0].bar(x, recalls, width, label='Recall', color='lightcoral')\n",
    "    axes[0].bar(x + width, f1_scores, width, label='F1 Score', color='lightgreen')\n",
    "    axes[0].set_xlabel('Tags')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('Performance Metrics by Tag')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(tag_names_eval)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim([0, 1.1])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Radar chart\n",
    "    if len(tag_metrics) >= 3:\n",
    "        angles = np.linspace(0, 2 * np.pi, len(tag_names_eval), endpoint=False).tolist()\n",
    "        precisions_plot = precisions + [precisions[0]]\n",
    "        recalls_plot = recalls + [recalls[0]]\n",
    "        angles_plot = angles + [angles[0]]\n",
    "        \n",
    "        ax = plt.subplot(122, projection='polar')\n",
    "        ax.plot(angles_plot, precisions_plot, 'o-', linewidth=2, label='Precision', color='skyblue')\n",
    "        ax.fill(angles_plot, precisions_plot, alpha=0.25, color='skyblue')\n",
    "        ax.plot(angles_plot, recalls_plot, 'o-', linewidth=2, label='Recall', color='lightcoral')\n",
    "        ax.fill(angles_plot, recalls_plot, alpha=0.25, color='lightcoral')\n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(tag_names_eval)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('Performance Radar Chart')\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return performance\n",
    "\n",
    "performance = evaluate_model_advanced(project.id, iteration.id, training_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix Analysis\n",
    "\n",
    "A confusion matrix helps visualize where the model makes mistakes. Let's create one by testing on multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(test_folder, project_id, publish_name, prediction_client, tags_dict):\n",
    "    \"\"\"\n",
    "    Create confusion matrix by testing on images organized in folders by true label.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    print(\"Testing model and building confusion matrix...\\n\")\n",
    "    \n",
    "    for tag_name in tags_dict.keys():\n",
    "        tag_folder = os.path.join(test_folder, tag_name)\n",
    "        \n",
    "        if not os.path.exists(tag_folder):\n",
    "            continue\n",
    "        \n",
    "        test_images = [f for f in os.listdir(tag_folder) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        \n",
    "        for test_image in test_images:\n",
    "            image_path = os.path.join(tag_folder, test_image)\n",
    "            \n",
    "            with open(image_path, \"rb\") as image_data:\n",
    "                results = prediction_client.classify_image(project_id, publish_name, image_data.read())\n",
    "            \n",
    "            # Get top prediction\n",
    "            if results.predictions:\n",
    "                top_prediction = max(results.predictions, key=lambda p: p.probability)\n",
    "                y_true.append(tag_name)\n",
    "                y_pred.append(top_prediction.tag_name)\n",
    "    \n",
    "    if not y_true:\n",
    "        print(\"No test images found\")\n",
    "        return\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    labels = sorted(tags_dict.keys())\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(classification_report(y_true, y_pred, labels=labels, target_names=labels))\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Create confusion matrix if test images are available\n",
    "test_images_path = 'test-images'\n",
    "if os.path.exists(test_images_path):\n",
    "    cm = create_confusion_matrix(test_images_path, project.id, publish_name, prediction_client, tags)\n",
    "else:\n",
    "    print(f\"Test images not found at {test_images_path}\")\n",
    "    print(\"Organize test images in folders by category to generate confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-Label Classification\n",
    "\n",
    "Unlike multi-class classification (one label per image), **multi-label classification** allows assigning multiple tags to a single image.\n",
    "\n",
    "For example, an image might contain both an apple and a banana.\n",
    "\n",
    "### Creating a Multi-Label Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-label classification project\n",
    "multilabel_project_name = f\"Multi-Label Fruit {uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "multilabel_project = training_client.create_project(\n",
    "    name=multilabel_project_name,\n",
    "    description=\"Multi-label fruit classification - images can have multiple fruit types\",\n",
    "    domain_id=food_domain.id,\n",
    "    classification_type=\"Multilabel\"  # Key difference!\n",
    ")\n",
    "\n",
    "print(f\"✓ Created multi-label project: {multilabel_project.name}\")\n",
    "print(f\"  Project ID: {multilabel_project.id}\")\n",
    "print(f\"  Classification Type: Multilabel\")\n",
    "print(f\"\\nIn this project, images can be tagged with multiple labels simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Predictions\n",
    "\n",
    "For processing multiple images efficiently, use batch predictions instead of individual calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict_images(image_folder, project_id, publish_name, prediction_client, \n",
    "                        confidence_threshold=0.5, max_images=10):\n",
    "    \"\"\"\n",
    "    Perform batch predictions on multiple images.\n",
    "    \n",
    "    Args:\n",
    "        image_folder: Path to folder containing images\n",
    "        project_id: Custom Vision project ID\n",
    "        publish_name: Published model name\n",
    "        prediction_client: Authenticated prediction client\n",
    "        confidence_threshold: Minimum confidence to report\n",
    "        max_images: Maximum number of images to process\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of results\n",
    "    \"\"\"\n",
    "    results_dict = defaultdict(list)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_folder) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "    \n",
    "    print(f\"Processing {min(len(image_files), max_images)} images...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files[:max_images]):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        \n",
    "        with open(image_path, \"rb\") as image_data:\n",
    "            results = prediction_client.classify_image(\n",
    "                project_id, \n",
    "                publish_name, \n",
    "                image_data.read()\n",
    "            )\n",
    "        \n",
    "        # Store results\n",
    "        top_prediction = max(results.predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        if top_prediction.probability >= confidence_threshold:\n",
    "            results_dict[top_prediction.tag_name].append({\n",
    "                'file': image_file,\n",
    "                'confidence': top_prediction.probability\n",
    "            })\n",
    "            status = \"✓\"\n",
    "        else:\n",
    "            results_dict['low_confidence'].append({\n",
    "                'file': image_file,\n",
    "                'confidence': top_prediction.probability,\n",
    "                'predicted': top_prediction.tag_name\n",
    "            })\n",
    "            status = \"⚠\"\n",
    "        \n",
    "        print(f\"{status} {idx+1}/{min(len(image_files), max_images)}: {image_file} -> \"\n",
    "              f\"{top_prediction.tag_name} ({top_prediction.probability:.2%})\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Batch Processing Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total images processed: {min(len(image_files), max_images)}\")\n",
    "    print(f\"Total time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Average time per image: {elapsed_time / min(len(image_files), max_images):.2f} seconds\")\n",
    "    print(f\"\\nResults by category:\")\n",
    "    \n",
    "    for category, items in results_dict.items():\n",
    "        if category != 'low_confidence':\n",
    "            avg_conf = np.mean([item['confidence'] for item in items])\n",
    "            print(f\"  {category}: {len(items)} images (avg confidence: {avg_conf:.2%})\")\n",
    "    \n",
    "    if 'low_confidence' in results_dict:\n",
    "        print(f\"  Low confidence: {len(results_dict['low_confidence'])} images\")\n",
    "    \n",
    "    return dict(results_dict)\n",
    "\n",
    "# Run batch predictions\n",
    "if os.path.exists(test_images_path):\n",
    "    batch_results = batch_predict_images(\n",
    "        test_images_path, \n",
    "        project.id, \n",
    "        publish_name, \n",
    "        prediction_client,\n",
    "        confidence_threshold=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Export for Edge Deployment\n",
    "\n",
    "Azure Custom Vision allows exporting models for edge deployment, enabling offline predictions on devices.\n",
    "\n",
    "### Supported Export Formats:\n",
    "- **TensorFlow**: For general ML applications\n",
    "- **CoreML**: For iOS devices\n",
    "- **ONNX**: Cross-platform format\n",
    "- **Dockerfile**: Containerized deployment\n",
    "\n",
    "**Note**: Only models trained with \"compact\" domains can be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(project_id, iteration_id, platform, training_client):\n",
    "    \"\"\"\n",
    "    Export model for edge deployment.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Custom Vision project ID\n",
    "        iteration_id: Iteration to export\n",
    "        platform: Export platform (e.g., 'TensorFlow', 'CoreML', 'ONNX')\n",
    "        training_client: Authenticated training client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to export model to {platform} format...\")\n",
    "        \n",
    "        # Check if iteration can be exported\n",
    "        iteration_details = training_client.get_iteration(project_id, iteration_id)\n",
    "        \n",
    "        if not iteration_details.exportable:\n",
    "            print(f\"\\n⚠️  This iteration cannot be exported.\")\n",
    "            print(f\"To export models, create a project with a 'compact' domain.\")\n",
    "            print(f\"\\nAvailable compact domains:\")\n",
    "            \n",
    "            domains = training_client.get_domains()\n",
    "            for domain in domains:\n",
    "                if 'compact' in domain.name.lower() and domain.exportable:\n",
    "                    print(f\"  - {domain.name}\")\n",
    "            return\n",
    "        \n",
    "        # Export the model\n",
    "        export = training_client.export_iteration(project_id, iteration_id, platform)\n",
    "        \n",
    "        # Wait for export to complete\n",
    "        while export.status == \"Exporting\":\n",
    "            print(f\"Export status: {export.status}...\")\n",
    "            time.sleep(5)\n",
    "            export = training_client.get_exports(project_id, iteration_id)[0]\n",
    "        \n",
    "        if export.status == \"Done\":\n",
    "            print(f\"\\n✓ Model exported successfully!\")\n",
    "            print(f\"  Download URL: {export.download_uri}\")\n",
    "            print(f\"  Platform: {export.platform}\")\n",
    "            print(f\"\\n⚠️  Download link expires after a certain time period.\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Export failed with status: {export.status}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "# Attempt to export (will show instructions if not using compact domain)\n",
    "export_model(project.id, iteration.id, 'TensorFlow', training_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Optimization Strategies\n",
    "\n",
    "### Probability Threshold Tuning\n",
    "\n",
    "Adjusting the probability threshold affects the precision-recall tradeoff:\n",
    "- **Higher threshold** (e.g., 0.8): Higher precision, lower recall (fewer false positives)\n",
    "- **Lower threshold** (e.g., 0.3): Higher recall, lower precision (fewer false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_threshold_impact(test_folder, project_id, publish_name, prediction_client, tags_dict):\n",
    "    \"\"\"\n",
    "    Analyze the impact of different probability thresholds on classification.\n",
    "    \"\"\"\n",
    "    # Collect all predictions\n",
    "    all_predictions = []\n",
    "    \n",
    "    for tag_name in tags_dict.keys():\n",
    "        tag_folder = os.path.join(test_folder, tag_name)\n",
    "        \n",
    "        if not os.path.exists(tag_folder):\n",
    "            continue\n",
    "        \n",
    "        test_images = [f for f in os.listdir(tag_folder) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        \n",
    "        for test_image in test_images[:5]:  # Limit for demo\n",
    "            image_path = os.path.join(tag_folder, test_image)\n",
    "            \n",
    "            with open(image_path, \"rb\") as image_data:\n",
    "                results = prediction_client.classify_image(project_id, publish_name, image_data.read())\n",
    "            \n",
    "            if results.predictions:\n",
    "                top_pred = max(results.predictions, key=lambda p: p.probability)\n",
    "                all_predictions.append({\n",
    "                    'true_label': tag_name,\n",
    "                    'predicted_label': top_pred.tag_name,\n",
    "                    'confidence': top_pred.probability\n",
    "                })\n",
    "    \n",
    "    if not all_predictions:\n",
    "        print(\"No predictions to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "    results_by_threshold = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        rejected = 0\n",
    "        \n",
    "        for pred in all_predictions:\n",
    "            if pred['confidence'] >= threshold:\n",
    "                total += 1\n",
    "                if pred['true_label'] == pred['predicted_label']:\n",
    "                    correct += 1\n",
    "            else:\n",
    "                rejected += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        acceptance_rate = total / len(all_predictions)\n",
    "        \n",
    "        results_by_threshold.append({\n",
    "            'threshold': threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'acceptance_rate': acceptance_rate,\n",
    "            'rejected': rejected\n",
    "        })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Threshold Impact Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Threshold':<12} {'Accuracy':<12} {'Acceptance':<15} {'Rejected'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for result in results_by_threshold:\n",
    "        print(f\"{result['threshold']:<12.1f} {result['accuracy']:<12.2%} \"\n",
    "              f\"{result['acceptance_rate']:<15.2%} {result['rejected']}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax1.set_xlabel('Confidence Threshold')\n",
    "    ax1.set_ylabel('Accuracy', color='blue')\n",
    "    ax1.plot([r['threshold'] for r in results_by_threshold],\n",
    "             [r['accuracy'] for r in results_by_threshold],\n",
    "             'o-', color='blue', linewidth=2, markersize=8, label='Accuracy')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Acceptance Rate', color='red')\n",
    "    ax2.plot([r['threshold'] for r in results_by_threshold],\n",
    "             [r['acceptance_rate'] for r in results_by_threshold],\n",
    "             's-', color='red', linewidth=2, markersize=8, label='Acceptance Rate')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    plt.title('Threshold Impact: Accuracy vs Acceptance Rate')\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Analyze threshold impact\n",
    "if os.path.exists(test_images_path):\n",
    "    analyze_threshold_impact(test_images_path, project.id, publish_name, prediction_client, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Production Deployment Best Practices\n",
    "\n",
    "### Key Considerations:\n",
    "\n",
    "1. **Model Versioning**: Track iterations and publish names\n",
    "2. **Monitoring**: Log predictions and confidence scores\n",
    "3. **Fallback Logic**: Handle low-confidence predictions\n",
    "4. **Continuous Improvement**: Collect production data for retraining\n",
    "5. **Error Handling**: Implement retry logic and timeouts\n",
    "6. **Performance**: Use batch predictions when possible\n",
    "7. **Security**: Protect API keys, use managed identities\n",
    "8. **Cost Management**: Monitor API usage and optimize calls\n",
    "\n",
    "### Example Production Code Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionClassifier:\n",
    "    \"\"\"\n",
    "    Production-ready image classifier with error handling and logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint, key, project_id, model_name, \n",
    "                 confidence_threshold=0.7, retry_attempts=3):\n",
    "        self.endpoint = endpoint\n",
    "        self.project_id = project_id\n",
    "        self.model_name = model_name\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.retry_attempts = retry_attempts\n",
    "        \n",
    "        credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": key})\n",
    "        self.client = CustomVisionPredictionClient(endpoint, credentials)\n",
    "        \n",
    "        # Tracking\n",
    "        self.prediction_count = 0\n",
    "        self.low_confidence_count = 0\n",
    "        self.error_count = 0\n",
    "    \n",
    "    def classify(self, image_data, return_all_predictions=False):\n",
    "        \"\"\"\n",
    "        Classify an image with error handling and retry logic.\n",
    "        \n",
    "        Args:\n",
    "            image_data: Image bytes\n",
    "            return_all_predictions: Whether to return all predictions or just top one\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with prediction results and metadata\n",
    "        \"\"\"\n",
    "        for attempt in range(self.retry_attempts):\n",
    "            try:\n",
    "                results = self.client.classify_image(\n",
    "                    self.project_id, \n",
    "                    self.model_name, \n",
    "                    image_data\n",
    "                )\n",
    "                \n",
    "                self.prediction_count += 1\n",
    "                \n",
    "                if not results.predictions:\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'error': 'No predictions returned',\n",
    "                        'confidence': 0\n",
    "                    }\n",
    "                \n",
    "                top_prediction = max(results.predictions, key=lambda p: p.probability)\n",
    "                \n",
    "                # Check confidence\n",
    "                if top_prediction.probability < self.confidence_threshold:\n",
    "                    self.low_confidence_count += 1\n",
    "                \n",
    "                if return_all_predictions:\n",
    "                    predictions = [\n",
    "                        {'label': p.tag_name, 'confidence': p.probability}\n",
    "                        for p in results.predictions\n",
    "                    ]\n",
    "                else:\n",
    "                    predictions = {\n",
    "                        'label': top_prediction.tag_name,\n",
    "                        'confidence': top_prediction.probability\n",
    "                    }\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'prediction': predictions,\n",
    "                    'meets_threshold': top_prediction.probability >= self.confidence_threshold,\n",
    "                    'iteration_id': results.iteration\n",
    "                }\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.error_count += 1\n",
    "                if attempt == self.retry_attempts - 1:\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'error': str(e),\n",
    "                        'attempts': attempt + 1\n",
    "                    }\n",
    "                time.sleep(1 * (attempt + 1))  # Exponential backoff\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get classifier statistics.\"\"\"\n",
    "        return {\n",
    "            'total_predictions': self.prediction_count,\n",
    "            'low_confidence': self.low_confidence_count,\n",
    "            'low_confidence_rate': self.low_confidence_count / max(1, self.prediction_count),\n",
    "            'errors': self.error_count,\n",
    "            'error_rate': self.error_count / max(1, self.prediction_count + self.error_count)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"Production Classifier Example:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "prod_classifier = ProductionClassifier(\n",
    "    prediction_endpoint,\n",
    "    prediction_key,\n",
    "    project.id,\n",
    "    publish_name,\n",
    "    confidence_threshold=0.7\n",
    ")\n",
    "\n",
    "# Test with a few images\n",
    "if os.path.exists(test_images_path):\n",
    "    test_files = [f for f in os.listdir(test_images_path) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:3]\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        with open(os.path.join(test_images_path, test_file), 'rb') as f:\n",
    "            image_data = f.read()\n",
    "        \n",
    "        result = prod_classifier.classify(image_data)\n",
    "        \n",
    "        if result['success']:\n",
    "            pred = result['prediction']\n",
    "            threshold_met = \"✓\" if result['meets_threshold'] else \"⚠\"\n",
    "            print(f\"{threshold_met} {test_file}: {pred['label']} ({pred['confidence']:.2%})\")\n",
    "        else:\n",
    "            print(f\"❌ {test_file}: Error - {result['error']}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    stats = prod_classifier.get_stats()\n",
    "    print(f\"\\nClassifier Statistics:\")\n",
    "    print(f\"  Total predictions: {stats['total_predictions']}\")\n",
    "    print(f\"  Low confidence: {stats['low_confidence']} ({stats['low_confidence_rate']:.1%})\")\n",
    "    print(f\"  Errors: {stats['errors']} ({stats['error_rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "### Advanced Concepts Covered:\n",
    "✓ Transfer learning and domain selection  \n",
    "✓ Data augmentation for improved performance  \n",
    "✓ Advanced evaluation metrics (F1, confusion matrix)  \n",
    "✓ Multi-label vs multi-class classification  \n",
    "✓ Batch predictions for efficiency  \n",
    "✓ Model export for edge deployment  \n",
    "✓ Threshold tuning for precision-recall tradeoff  \n",
    "✓ Production-ready deployment patterns  \n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Domain selection** can significantly impact model performance\n",
    "2. **Data augmentation** helps with limited training data\n",
    "3. **Evaluation metrics** beyond accuracy provide deeper insights\n",
    "4. **Threshold tuning** allows balancing precision and recall\n",
    "5. **Production deployment** requires error handling and monitoring\n",
    "\n",
    "### Next Steps:\n",
    "- Explore Lab 05: Object Detection for locating and classifying objects\n",
    "- Experiment with different domains and augmentation strategies\n",
    "- Implement active learning to improve models over time\n",
    "- Deploy models to edge devices using exported formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete projects\n",
    "# training_client.delete_project(project.id)\n",
    "# training_client.delete_project(multilabel_project.id)\n",
    "# print(\"Projects deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
